{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3QfwOIDnl9T_",
        "outputId": "b4ed1909-09ad-4005-b142-ebe8c9736115"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Unnamed: 0  \\\n",
              "0           0   \n",
              "1           1   \n",
              "2           2   \n",
              "3           3   \n",
              "4           4   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 abstract  \n",
              "0                                                                                                          Transformers have shown great potential in computer vision tasks. A common\\nbelief is their attention-based token mixer module contributes most to their\\ncompetence. However, recent works show the attention-based module in\\ntransformers can be replaced by spatial MLPs and the resulted models still\\nperform quite well. Based on this observation, we hypothesize that the general\\narchitecture of the transformers, instead of the specific token mixer module,\\nis more essential to the model's performance. To verify this, we deliberately\\nreplace the attention module in transformers with an embarrassingly simple\\nspatial pooling operator to conduct only the most basic token mixing.\\nSurprisingly, we observe that the derived model, termed as PoolFormer, achieves\\ncompetitive performance on multiple computer vision tasks. For example, on\\nImageNet-1K, PoolFormer achieves 82.1% top-1 accuracy, surpassing well-tuned\\nvision transformer/MLP-like baselines DeiT-B/ResMLP-B24 by 0.3%/1.1% accuracy\\nwith 35%/52% fewer parameters and 48%/60% fewer MACs. The effectiveness of\\nPoolFormer verifies our hypothesis and urges us to initiate the concept of\\n\"MetaFormer\", a general architecture abstracted from transformers without\\nspecifying the token mixer. Based on the extensive experiments, we argue that\\nMetaFormer is the key player in achieving superior results for recent\\ntransformer and MLP-like models on vision tasks. This work calls for more\\nfuture research dedicated to improving MetaFormer instead of focusing on the\\ntoken mixer modules. Additionally, our proposed PoolFormer could serve as a\\nstarting baseline for future MetaFormer architecture design. Code is available\\nat https://github.com/sail-sg/poolformer  \n",
              "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 A critical aspect of reliable communication involves the design of codes that\\nallow transmissions to be robustly and computationally efficiently decoded\\nunder noisy conditions. Advances in the design of reliable codes have been\\ndriven by coding theory and have been sporadic. Recently, it is shown that\\nchannel codes that are comparable to modern codes can be learned solely via\\ndeep learning. In particular, Turbo Autoencoder (TURBOAE), introduced by Jiang\\net al., is shown to achieve the reliability of Turbo codes for Additive White\\nGaussian Noise channels. In this paper, we focus on applying the idea of\\nTURBOAE to various practical channels, such as fading channels and chirp noise\\nchannels. We introduce TURBOAE-TI, a novel neural architecture that combines\\nTURBOAE with a trainable interleaver design. We develop a carefully-designed\\ntraining procedure and a novel interleaver penalty function that are crucial in\\nlearning the interleaver and TURBOAE jointly. We demonstrate that TURBOAE-TI\\noutperforms TURBOAE and LTE Turbo codes for several channels of interest. We\\nalso provide interpretation analysis to better understand TURBOAE-TI.  \n",
              "2  Point defects are responsible for a wide range of optoelectronic properties\\nin materials, making it crucial to engineer their concentrations for novel\\nmaterials design. However, considering the plethora of defects in co-doped\\nsemiconducting and dielectric materials and the dependence of defect formation\\nenergies on heat treatment parameters, process design based on an experimental\\ntrial and error approach is not an efficient strategy. This makes it necessary\\nto explore computational pathways for predicting defect equilibria during heat\\ntreatments. The accumulated experimental knowledge on defect transformations in\\ndiamond is unparalleled. Therefore, diamond is an excellent material for\\nbenchmarking computational approaches. By considering nitrogen, hydrogen, and\\nsilicon doped diamond as a model system, we have investigated the pressure\\ndependence of defect formation energies and calculated the defect equilibria\\nduring heat treatment of diamond through ab-initio calculations. We have\\nplotted monolithic-Kr\\\"oger-Vink diagrams for various defects, representing\\ndefect concentrations based on process parameters, such as temperature and\\npartial pressure of gases used during heat treatments of diamond. The method\\ndemonstrated predicts the majority of experimental data, such as nitrogen\\naggregation path leading towards the formation of the B center, annealing of\\nthe B, H3, N3, and NVHx centers at ultra high temperatures, the thermal\\nstability of the SiV center, and temperature dependence of NV concentration. We\\ndemonstrate the possibility of designing heat treatments for a wide range of\\nsemiconducting and dielectric materials by using a relatively inexpensive yet\\nrobust first principles approach, significantly accelerating defect engineering\\nand high-throughput novel materials design.  \n",
              "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                We investigate the electrostrictive response across a ferroelectric phase\\ntransition from first-principles calculations and refute the prevailing view of\\nconstant electrostriction across the ferroelectric phase boundary. We take as a\\ncase study the epitaxial strain-induced transition from para- to\\nferoelectricity of \\ce{KTaO3}. We show that the magnitude of the\\nelectrostriction diverges with the permitivity at the transition, hence\\nexhibiting giant responses through a calculation of both the M and Q\\nelectrostrictive tensors. We explain the origin of this giant electrostrictive\\nresponse in \\ce{KTaO3} using a microscopic decomposition of the\\nelectrostriction coefficients, and use this understanding to propose design\\nrules for the development of future giant electrostrictors for\\nelectromechanical applications. Finally, we introduce a further means to\\ncalculate electrostriction, specific to ferroelectrics, and not yet utilised in\\nthe literature.  \n",
              "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Institutions in highly regulated domains such as finance and healthcare often\\nhave restrictive rules around data sharing. Federated learning is a distributed\\nlearning framework that enables multi-institutional collaborations on\\ndecentralized data with improved protection for each collaborator's data\\nprivacy. In this paper, we propose a communication-efficient scheme for\\ndecentralized federated learning called ProxyFL, or proxy-based federated\\nlearning. Each participant in ProxyFL maintains two models, a private model,\\nand a publicly shared proxy model designed to protect the participant's\\nprivacy. Proxy models allow efficient information exchange among participants\\nusing the PushSum method without the need of a centralized server. The proposed\\nmethod eliminates a significant limitation of canonical federated learning by\\nallowing model heterogeneity; each participant can have a private model with\\nany architecture. Furthermore, our protocol for communication by proxy leads to\\nstronger privacy guarantees using differential privacy analysis. Experiments on\\npopular image datasets, and a pan-cancer diagnostic problem using over 30,000\\nhigh-quality gigapixel histology whole slide images, show that ProxyFL can\\noutperform existing alternatives with much less communication overhead and\\nstronger privacy.  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-78ccaac0-7860-4ed3-a76e-9ade60dc6c7a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>abstract</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Transformers have shown great potential in computer vision tasks. A common\\nbelief is their attention-based token mixer module contributes most to their\\ncompetence. However, recent works show the attention-based module in\\ntransformers can be replaced by spatial MLPs and the resulted models still\\nperform quite well. Based on this observation, we hypothesize that the general\\narchitecture of the transformers, instead of the specific token mixer module,\\nis more essential to the model's performance. To verify this, we deliberately\\nreplace the attention module in transformers with an embarrassingly simple\\nspatial pooling operator to conduct only the most basic token mixing.\\nSurprisingly, we observe that the derived model, termed as PoolFormer, achieves\\ncompetitive performance on multiple computer vision tasks. For example, on\\nImageNet-1K, PoolFormer achieves 82.1% top-1 accuracy, surpassing well-tuned\\nvision transformer/MLP-like baselines DeiT-B/ResMLP-B24 by 0.3%/1.1% accuracy\\nwith 35%/52% fewer parameters and 48%/60% fewer MACs. The effectiveness of\\nPoolFormer verifies our hypothesis and urges us to initiate the concept of\\n\"MetaFormer\", a general architecture abstracted from transformers without\\nspecifying the token mixer. Based on the extensive experiments, we argue that\\nMetaFormer is the key player in achieving superior results for recent\\ntransformer and MLP-like models on vision tasks. This work calls for more\\nfuture research dedicated to improving MetaFormer instead of focusing on the\\ntoken mixer modules. Additionally, our proposed PoolFormer could serve as a\\nstarting baseline for future MetaFormer architecture design. Code is available\\nat https://github.com/sail-sg/poolformer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>A critical aspect of reliable communication involves the design of codes that\\nallow transmissions to be robustly and computationally efficiently decoded\\nunder noisy conditions. Advances in the design of reliable codes have been\\ndriven by coding theory and have been sporadic. Recently, it is shown that\\nchannel codes that are comparable to modern codes can be learned solely via\\ndeep learning. In particular, Turbo Autoencoder (TURBOAE), introduced by Jiang\\net al., is shown to achieve the reliability of Turbo codes for Additive White\\nGaussian Noise channels. In this paper, we focus on applying the idea of\\nTURBOAE to various practical channels, such as fading channels and chirp noise\\nchannels. We introduce TURBOAE-TI, a novel neural architecture that combines\\nTURBOAE with a trainable interleaver design. We develop a carefully-designed\\ntraining procedure and a novel interleaver penalty function that are crucial in\\nlearning the interleaver and TURBOAE jointly. We demonstrate that TURBOAE-TI\\noutperforms TURBOAE and LTE Turbo codes for several channels of interest. We\\nalso provide interpretation analysis to better understand TURBOAE-TI.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>Point defects are responsible for a wide range of optoelectronic properties\\nin materials, making it crucial to engineer their concentrations for novel\\nmaterials design. However, considering the plethora of defects in co-doped\\nsemiconducting and dielectric materials and the dependence of defect formation\\nenergies on heat treatment parameters, process design based on an experimental\\ntrial and error approach is not an efficient strategy. This makes it necessary\\nto explore computational pathways for predicting defect equilibria during heat\\ntreatments. The accumulated experimental knowledge on defect transformations in\\ndiamond is unparalleled. Therefore, diamond is an excellent material for\\nbenchmarking computational approaches. By considering nitrogen, hydrogen, and\\nsilicon doped diamond as a model system, we have investigated the pressure\\ndependence of defect formation energies and calculated the defect equilibria\\nduring heat treatment of diamond through ab-initio calculations. We have\\nplotted monolithic-Kr\\\"oger-Vink diagrams for various defects, representing\\ndefect concentrations based on process parameters, such as temperature and\\npartial pressure of gases used during heat treatments of diamond. The method\\ndemonstrated predicts the majority of experimental data, such as nitrogen\\naggregation path leading towards the formation of the B center, annealing of\\nthe B, H3, N3, and NVHx centers at ultra high temperatures, the thermal\\nstability of the SiV center, and temperature dependence of NV concentration. We\\ndemonstrate the possibility of designing heat treatments for a wide range of\\nsemiconducting and dielectric materials by using a relatively inexpensive yet\\nrobust first principles approach, significantly accelerating defect engineering\\nand high-throughput novel materials design.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>We investigate the electrostrictive response across a ferroelectric phase\\ntransition from first-principles calculations and refute the prevailing view of\\nconstant electrostriction across the ferroelectric phase boundary. We take as a\\ncase study the epitaxial strain-induced transition from para- to\\nferoelectricity of \\ce{KTaO3}. We show that the magnitude of the\\nelectrostriction diverges with the permitivity at the transition, hence\\nexhibiting giant responses through a calculation of both the M and Q\\nelectrostrictive tensors. We explain the origin of this giant electrostrictive\\nresponse in \\ce{KTaO3} using a microscopic decomposition of the\\nelectrostriction coefficients, and use this understanding to propose design\\nrules for the development of future giant electrostrictors for\\nelectromechanical applications. Finally, we introduce a further means to\\ncalculate electrostriction, specific to ferroelectrics, and not yet utilised in\\nthe literature.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>Institutions in highly regulated domains such as finance and healthcare often\\nhave restrictive rules around data sharing. Federated learning is a distributed\\nlearning framework that enables multi-institutional collaborations on\\ndecentralized data with improved protection for each collaborator's data\\nprivacy. In this paper, we propose a communication-efficient scheme for\\ndecentralized federated learning called ProxyFL, or proxy-based federated\\nlearning. Each participant in ProxyFL maintains two models, a private model,\\nand a publicly shared proxy model designed to protect the participant's\\nprivacy. Proxy models allow efficient information exchange among participants\\nusing the PushSum method without the need of a centralized server. The proposed\\nmethod eliminates a significant limitation of canonical federated learning by\\nallowing model heterogeneity; each participant can have a private model with\\nany architecture. Furthermore, our protocol for communication by proxy leads to\\nstronger privacy guarantees using differential privacy analysis. Experiments on\\npopular image datasets, and a pan-cancer diagnostic problem using over 30,000\\nhigh-quality gigapixel histology whole slide images, show that ProxyFL can\\noutperform existing alternatives with much less communication overhead and\\nstronger privacy.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-78ccaac0-7860-4ed3-a76e-9ade60dc6c7a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-78ccaac0-7860-4ed3-a76e-9ade60dc6c7a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-78ccaac0-7860-4ed3-a76e-9ade60dc6c7a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ],
      "source": [
        "import pandas as pd\n",
        " \n",
        "pd.set_option('display.max_colwidth', None)\n",
        "data = pd.read_csv('/content/drive/MyDrive/AI Datasets/combined.csv')\n",
        "data.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z_NDR-GYoG3Z",
        "outputId": "bcc06a2d-161d-4cee-a8f3-d7e11ce8bce5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(49801, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.dropna(axis=0,inplace=True)"
      ],
      "metadata": {
        "id": "-hC0EnruokPo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5rCe0iimotp4",
        "outputId": "4c8aa2b8-925c-47f2-97f2-302d3b241b33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(49801, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.rename(columns = {'Unnamed: 0':'id'}, inplace = True)\n",
        "data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 961
        },
        "id": "P0IXLcreovSS",
        "outputId": "7fbca9ce-38bd-436b-afa9-228515eaf6f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   id  \\\n",
              "0   0   \n",
              "1   1   \n",
              "2   2   \n",
              "3   3   \n",
              "4   4   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 abstract  \n",
              "0                                                                                                          Transformers have shown great potential in computer vision tasks. A common\\nbelief is their attention-based token mixer module contributes most to their\\ncompetence. However, recent works show the attention-based module in\\ntransformers can be replaced by spatial MLPs and the resulted models still\\nperform quite well. Based on this observation, we hypothesize that the general\\narchitecture of the transformers, instead of the specific token mixer module,\\nis more essential to the model's performance. To verify this, we deliberately\\nreplace the attention module in transformers with an embarrassingly simple\\nspatial pooling operator to conduct only the most basic token mixing.\\nSurprisingly, we observe that the derived model, termed as PoolFormer, achieves\\ncompetitive performance on multiple computer vision tasks. For example, on\\nImageNet-1K, PoolFormer achieves 82.1% top-1 accuracy, surpassing well-tuned\\nvision transformer/MLP-like baselines DeiT-B/ResMLP-B24 by 0.3%/1.1% accuracy\\nwith 35%/52% fewer parameters and 48%/60% fewer MACs. The effectiveness of\\nPoolFormer verifies our hypothesis and urges us to initiate the concept of\\n\"MetaFormer\", a general architecture abstracted from transformers without\\nspecifying the token mixer. Based on the extensive experiments, we argue that\\nMetaFormer is the key player in achieving superior results for recent\\ntransformer and MLP-like models on vision tasks. This work calls for more\\nfuture research dedicated to improving MetaFormer instead of focusing on the\\ntoken mixer modules. Additionally, our proposed PoolFormer could serve as a\\nstarting baseline for future MetaFormer architecture design. Code is available\\nat https://github.com/sail-sg/poolformer  \n",
              "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 A critical aspect of reliable communication involves the design of codes that\\nallow transmissions to be robustly and computationally efficiently decoded\\nunder noisy conditions. Advances in the design of reliable codes have been\\ndriven by coding theory and have been sporadic. Recently, it is shown that\\nchannel codes that are comparable to modern codes can be learned solely via\\ndeep learning. In particular, Turbo Autoencoder (TURBOAE), introduced by Jiang\\net al., is shown to achieve the reliability of Turbo codes for Additive White\\nGaussian Noise channels. In this paper, we focus on applying the idea of\\nTURBOAE to various practical channels, such as fading channels and chirp noise\\nchannels. We introduce TURBOAE-TI, a novel neural architecture that combines\\nTURBOAE with a trainable interleaver design. We develop a carefully-designed\\ntraining procedure and a novel interleaver penalty function that are crucial in\\nlearning the interleaver and TURBOAE jointly. We demonstrate that TURBOAE-TI\\noutperforms TURBOAE and LTE Turbo codes for several channels of interest. We\\nalso provide interpretation analysis to better understand TURBOAE-TI.  \n",
              "2  Point defects are responsible for a wide range of optoelectronic properties\\nin materials, making it crucial to engineer their concentrations for novel\\nmaterials design. However, considering the plethora of defects in co-doped\\nsemiconducting and dielectric materials and the dependence of defect formation\\nenergies on heat treatment parameters, process design based on an experimental\\ntrial and error approach is not an efficient strategy. This makes it necessary\\nto explore computational pathways for predicting defect equilibria during heat\\ntreatments. The accumulated experimental knowledge on defect transformations in\\ndiamond is unparalleled. Therefore, diamond is an excellent material for\\nbenchmarking computational approaches. By considering nitrogen, hydrogen, and\\nsilicon doped diamond as a model system, we have investigated the pressure\\ndependence of defect formation energies and calculated the defect equilibria\\nduring heat treatment of diamond through ab-initio calculations. We have\\nplotted monolithic-Kr\\\"oger-Vink diagrams for various defects, representing\\ndefect concentrations based on process parameters, such as temperature and\\npartial pressure of gases used during heat treatments of diamond. The method\\ndemonstrated predicts the majority of experimental data, such as nitrogen\\naggregation path leading towards the formation of the B center, annealing of\\nthe B, H3, N3, and NVHx centers at ultra high temperatures, the thermal\\nstability of the SiV center, and temperature dependence of NV concentration. We\\ndemonstrate the possibility of designing heat treatments for a wide range of\\nsemiconducting and dielectric materials by using a relatively inexpensive yet\\nrobust first principles approach, significantly accelerating defect engineering\\nand high-throughput novel materials design.  \n",
              "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                We investigate the electrostrictive response across a ferroelectric phase\\ntransition from first-principles calculations and refute the prevailing view of\\nconstant electrostriction across the ferroelectric phase boundary. We take as a\\ncase study the epitaxial strain-induced transition from para- to\\nferoelectricity of \\ce{KTaO3}. We show that the magnitude of the\\nelectrostriction diverges with the permitivity at the transition, hence\\nexhibiting giant responses through a calculation of both the M and Q\\nelectrostrictive tensors. We explain the origin of this giant electrostrictive\\nresponse in \\ce{KTaO3} using a microscopic decomposition of the\\nelectrostriction coefficients, and use this understanding to propose design\\nrules for the development of future giant electrostrictors for\\nelectromechanical applications. Finally, we introduce a further means to\\ncalculate electrostriction, specific to ferroelectrics, and not yet utilised in\\nthe literature.  \n",
              "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Institutions in highly regulated domains such as finance and healthcare often\\nhave restrictive rules around data sharing. Federated learning is a distributed\\nlearning framework that enables multi-institutional collaborations on\\ndecentralized data with improved protection for each collaborator's data\\nprivacy. In this paper, we propose a communication-efficient scheme for\\ndecentralized federated learning called ProxyFL, or proxy-based federated\\nlearning. Each participant in ProxyFL maintains two models, a private model,\\nand a publicly shared proxy model designed to protect the participant's\\nprivacy. Proxy models allow efficient information exchange among participants\\nusing the PushSum method without the need of a centralized server. The proposed\\nmethod eliminates a significant limitation of canonical federated learning by\\nallowing model heterogeneity; each participant can have a private model with\\nany architecture. Furthermore, our protocol for communication by proxy leads to\\nstronger privacy guarantees using differential privacy analysis. Experiments on\\npopular image datasets, and a pan-cancer diagnostic problem using over 30,000\\nhigh-quality gigapixel histology whole slide images, show that ProxyFL can\\noutperform existing alternatives with much less communication overhead and\\nstronger privacy.  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3a982ee2-a7cc-4af4-a011-c42199ed8308\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>abstract</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Transformers have shown great potential in computer vision tasks. A common\\nbelief is their attention-based token mixer module contributes most to their\\ncompetence. However, recent works show the attention-based module in\\ntransformers can be replaced by spatial MLPs and the resulted models still\\nperform quite well. Based on this observation, we hypothesize that the general\\narchitecture of the transformers, instead of the specific token mixer module,\\nis more essential to the model's performance. To verify this, we deliberately\\nreplace the attention module in transformers with an embarrassingly simple\\nspatial pooling operator to conduct only the most basic token mixing.\\nSurprisingly, we observe that the derived model, termed as PoolFormer, achieves\\ncompetitive performance on multiple computer vision tasks. For example, on\\nImageNet-1K, PoolFormer achieves 82.1% top-1 accuracy, surpassing well-tuned\\nvision transformer/MLP-like baselines DeiT-B/ResMLP-B24 by 0.3%/1.1% accuracy\\nwith 35%/52% fewer parameters and 48%/60% fewer MACs. The effectiveness of\\nPoolFormer verifies our hypothesis and urges us to initiate the concept of\\n\"MetaFormer\", a general architecture abstracted from transformers without\\nspecifying the token mixer. Based on the extensive experiments, we argue that\\nMetaFormer is the key player in achieving superior results for recent\\ntransformer and MLP-like models on vision tasks. This work calls for more\\nfuture research dedicated to improving MetaFormer instead of focusing on the\\ntoken mixer modules. Additionally, our proposed PoolFormer could serve as a\\nstarting baseline for future MetaFormer architecture design. Code is available\\nat https://github.com/sail-sg/poolformer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>A critical aspect of reliable communication involves the design of codes that\\nallow transmissions to be robustly and computationally efficiently decoded\\nunder noisy conditions. Advances in the design of reliable codes have been\\ndriven by coding theory and have been sporadic. Recently, it is shown that\\nchannel codes that are comparable to modern codes can be learned solely via\\ndeep learning. In particular, Turbo Autoencoder (TURBOAE), introduced by Jiang\\net al., is shown to achieve the reliability of Turbo codes for Additive White\\nGaussian Noise channels. In this paper, we focus on applying the idea of\\nTURBOAE to various practical channels, such as fading channels and chirp noise\\nchannels. We introduce TURBOAE-TI, a novel neural architecture that combines\\nTURBOAE with a trainable interleaver design. We develop a carefully-designed\\ntraining procedure and a novel interleaver penalty function that are crucial in\\nlearning the interleaver and TURBOAE jointly. We demonstrate that TURBOAE-TI\\noutperforms TURBOAE and LTE Turbo codes for several channels of interest. We\\nalso provide interpretation analysis to better understand TURBOAE-TI.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>Point defects are responsible for a wide range of optoelectronic properties\\nin materials, making it crucial to engineer their concentrations for novel\\nmaterials design. However, considering the plethora of defects in co-doped\\nsemiconducting and dielectric materials and the dependence of defect formation\\nenergies on heat treatment parameters, process design based on an experimental\\ntrial and error approach is not an efficient strategy. This makes it necessary\\nto explore computational pathways for predicting defect equilibria during heat\\ntreatments. The accumulated experimental knowledge on defect transformations in\\ndiamond is unparalleled. Therefore, diamond is an excellent material for\\nbenchmarking computational approaches. By considering nitrogen, hydrogen, and\\nsilicon doped diamond as a model system, we have investigated the pressure\\ndependence of defect formation energies and calculated the defect equilibria\\nduring heat treatment of diamond through ab-initio calculations. We have\\nplotted monolithic-Kr\\\"oger-Vink diagrams for various defects, representing\\ndefect concentrations based on process parameters, such as temperature and\\npartial pressure of gases used during heat treatments of diamond. The method\\ndemonstrated predicts the majority of experimental data, such as nitrogen\\naggregation path leading towards the formation of the B center, annealing of\\nthe B, H3, N3, and NVHx centers at ultra high temperatures, the thermal\\nstability of the SiV center, and temperature dependence of NV concentration. We\\ndemonstrate the possibility of designing heat treatments for a wide range of\\nsemiconducting and dielectric materials by using a relatively inexpensive yet\\nrobust first principles approach, significantly accelerating defect engineering\\nand high-throughput novel materials design.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>We investigate the electrostrictive response across a ferroelectric phase\\ntransition from first-principles calculations and refute the prevailing view of\\nconstant electrostriction across the ferroelectric phase boundary. We take as a\\ncase study the epitaxial strain-induced transition from para- to\\nferoelectricity of \\ce{KTaO3}. We show that the magnitude of the\\nelectrostriction diverges with the permitivity at the transition, hence\\nexhibiting giant responses through a calculation of both the M and Q\\nelectrostrictive tensors. We explain the origin of this giant electrostrictive\\nresponse in \\ce{KTaO3} using a microscopic decomposition of the\\nelectrostriction coefficients, and use this understanding to propose design\\nrules for the development of future giant electrostrictors for\\nelectromechanical applications. Finally, we introduce a further means to\\ncalculate electrostriction, specific to ferroelectrics, and not yet utilised in\\nthe literature.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>Institutions in highly regulated domains such as finance and healthcare often\\nhave restrictive rules around data sharing. Federated learning is a distributed\\nlearning framework that enables multi-institutional collaborations on\\ndecentralized data with improved protection for each collaborator's data\\nprivacy. In this paper, we propose a communication-efficient scheme for\\ndecentralized federated learning called ProxyFL, or proxy-based federated\\nlearning. Each participant in ProxyFL maintains two models, a private model,\\nand a publicly shared proxy model designed to protect the participant's\\nprivacy. Proxy models allow efficient information exchange among participants\\nusing the PushSum method without the need of a centralized server. The proposed\\nmethod eliminates a significant limitation of canonical federated learning by\\nallowing model heterogeneity; each participant can have a private model with\\nany architecture. Furthermore, our protocol for communication by proxy leads to\\nstronger privacy guarantees using differential privacy analysis. Experiments on\\npopular image datasets, and a pan-cancer diagnostic problem using over 30,000\\nhigh-quality gigapixel histology whole slide images, show that ProxyFL can\\noutperform existing alternatives with much less communication overhead and\\nstronger privacy.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3a982ee2-a7cc-4af4-a011-c42199ed8308')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-3a982ee2-a7cc-4af4-a011-c42199ed8308 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-3a982ee2-a7cc-4af4-a011-c42199ed8308');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = data.head(100)"
      ],
      "metadata": {
        "id": "3y8lOrIJzlNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kYl9kVtRz1YY",
        "outputId": "3b05beb2-9e9d-4698-b8b9-3749ef876308"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(100, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.iloc[40]['abstract']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "uAlw3Jja_Jdp",
        "outputId": "c0be1e22-4ce7-4e06-b4af-42445488cdc9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Knowledge enriched language representation learning has shown promising\\nperformance across various knowledge-intensive NLP tasks. However, existing\\nknowledge based language models are all trained with monolingual knowledge\\ngraph data, which limits their application to more languages. In this work, we\\npresent a novel framework to pretrain knowledge based multilingual language\\nmodels (KMLMs). We first generate a large amount of code-switched synthetic\\nsentences and reasoning-based multilingual training data using the Wikidata\\nknowledge graphs. Then based on the intra- and inter-sentence structures of the\\ngenerated data, we design pretraining tasks to facilitate knowledge learning,\\nwhich allows the language models to not only memorize the factual knowledge but\\nalso learn useful logical patterns. Our pretrained KMLMs demonstrate\\nsignificant performance improvements on a wide range of knowledge-intensive\\ncross-lingual NLP tasks, including named entity recognition, factual knowledge\\nretrieval, relation classification, and a new task designed by us, namely,\\nlogic reasoning. Our code and pretrained language models will be made publicly\\navailable.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8yoqr9PMqOew",
        "outputId": "48a47b13-2f90-4eca-8698-cb3fc2658b67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.8/dist-packages (4.25.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.11.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.8.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tqdm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rHX4NyGgrZ5w",
        "outputId": "f516140f-0f0a-4bee-9736-3e3acca76a1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (4.64.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Useful libraries\n",
        "import numpy as np\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from keras_preprocessing.sequence import pad_sequences\n",
        "from transformers import BertTokenizer,  AutoModelForSequenceClassification\n",
        " \n",
        "# Load bert model\n",
        "model_path = \"bert-base-uncased\"\n",
        "tokenizer = BertTokenizer.from_pretrained(model_path,\n",
        "                                         do_lower_case=True)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_path,\n",
        "                                                         output_attentions=False,\n",
        "                                                         output_hidden_states=True)\n",
        "                                                        \n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        " \n",
        "def create_vector_from_text(tokenizer, model, text, MAX_LEN = 510):\n",
        "  \n",
        "    input_ids = tokenizer.encode(\n",
        "                        text,\n",
        "                        add_special_tokens = True,\n",
        "                        max_length = MAX_LEN,                          \n",
        "    )   \n",
        "    results = pad_sequences([input_ids], maxlen=MAX_LEN, dtype=\"long\",\n",
        "                              truncating=\"post\", padding=\"post\")\n",
        "    # Remove the outer list.\n",
        "    input_ids = results[0]\n",
        "    # Create attention masks   \n",
        "    attention_mask = [int(i>0) for i in input_ids]\n",
        "    # Convert to tensors.\n",
        "    input_ids = torch.tensor(input_ids)\n",
        "    attention_mask = torch.tensor(attention_mask)\n",
        "    # Add an extra dimension for the \"batch\" (even though there is only one\n",
        "    # input in this batch.)\n",
        "    input_ids = input_ids.unsqueeze(0)\n",
        "    attention_mask = attention_mask.unsqueeze(0)\n",
        "    # Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
        "    model.eval()\n",
        "    # Run the text through BERT, and collect all of the hidden states produced\n",
        "    # from all 12 layers.\n",
        "    with torch.no_grad():       \n",
        "        logits, encoded_layers = model(\n",
        "                                    input_ids = input_ids,\n",
        "                                    token_type_ids = None,\n",
        "                                    attention_mask = attention_mask,\n",
        "                                    return_dict=False)\n",
        "\n",
        "    layer_i = 12 # The last BERT layer before the classifier.\n",
        "    batch_i = 0 # Only one input in the batch.\n",
        "    token_i = 0 # The first token, corresponding to [CLS]\n",
        "      \n",
        "    # Extract the vector.\n",
        "    vector = encoded_layers[layer_i][batch_i][token_i]\n",
        "    # Move to the CPU and convert to numpy ndarray.\n",
        "    vector = vector.detach().cpu().numpy()\n",
        "    return(vector)\n",
        " \n",
        "def create_vector_index(data):\n",
        "  \n",
        "   # The list of all the vectors\n",
        "   vectors = []\n",
        "  \n",
        "   # Get overall text data\n",
        "   source_data = data.abstract.values\n",
        "  \n",
        "   # Loop over all the comment and get the embeddings\n",
        "   for text in tqdm(source_data):\n",
        "      \n",
        "       # Get the embedding\n",
        "       vector = create_vector_from_text(tokenizer, model, text)\n",
        "      \n",
        "       #add it to the list\n",
        "       vectors.append(vector)\n",
        "  \n",
        "   data[\"vectors\"] = vectors\n",
        "   data[\"vectors\"] = data[\"vectors\"].apply(lambda emb: np.array(emb))\n",
        "   data[\"vectors\"] = data[\"vectors\"].apply(lambda emb: emb.reshape(1, -1))\n",
        "   return data\n",
        "# Create the vector index\n",
        "vector_index = create_vector_index(data)\n",
        "vector_index.sample(5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "yaPZHv93pKdc",
        "outputId": "d4d2fd3b-2730-40f3-f7fe-377819003847"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "  0%|          | 0/100 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "100%|| 100/100 [03:24<00:00,  2.05s/it]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    id  \\\n",
              "29  29   \n",
              "68  68   \n",
              "9    9   \n",
              "39  39   \n",
              "3    3   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                abstract  \\\n",
              "29                                                                                                                                                                                                                                                                                                                                                                                                        We study the training of Vision Transformers for semi-supervised image\\nclassification. Transformers have recently demonstrated impressive performance\\non a multitude of supervised learning tasks. Surprisingly, we find Vision\\nTransformers perform poorly on a semi-supervised ImageNet setting. In contrast,\\nConvolutional Neural Networks (CNNs) achieve superior results in small labeled\\ndata regime. Further investigation reveals that the reason is CNNs have strong\\nspatial inductive bias. Inspired by this observation, we introduce a joint\\nsemi-supervised learning framework, Semiformer, which contains a Transformer\\nbranch, a Convolutional branch and a carefully designed fusion module for\\nknowledge sharing between the branches. The Convolutional branch is trained on\\nthe limited supervised data and generates pseudo labels to supervise the\\ntraining of the transformer branch on unlabeled data. Extensive experiments on\\nImageNet demonstrate that Semiformer achieves 75.5\\% top-1 accuracy,\\noutperforming the state-of-the-art. In addition, we show Semiformer is a\\ngeneral framework which is compatible with most modern Transformer and\\nConvolutional neural architectures.   \n",
              "68                                                                                                                                                                                                                                                                                                                                     Simultaneous wireless information and power transfer (SWIPT) has long been\\nproposed as a key solution for charging and communicating with low-cost and\\nlow-power devices. However, the employment of radio frequency (RF) signals for\\ninformation/power transfer needs to comply with international health and safety\\nregulations. In this paper, we provide a complete framework for the design and\\nanalysis of far-field SWIPT under safety constraints. In particular, we deal\\nwith two RF exposure regulations, namely, the specific absorption rate (SAR)\\nand the maximum permissible exposure (MPE). The state-of-the-art regarding SAR\\nand MPE is outlined together with a description as to how these can be modeled\\nin the context of communication networks. We propose a deep learning approach\\nfor the design of robust beamforming subject to specific information, energy\\nharvesting and SAR constraints. Furthermore, we present a thorough analytical\\nstudy for the performance of large-scale SWIPT systems, in terms of information\\nand energy coverage under MPE constraints. This work provides insights with\\nregards to the optimal SWIPT design as well as the potentials from the proper\\ndevelopment of SWIPT systems under health and safety restrictions.   \n",
              "9                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 In this paper, we present a robot model and code base for affordable\\neducation in the field of humanoid robotics. We give an overview of the\\nsoftware and hardware of a robot that won several competitions with the team\\nRoboKit in 2019-2021, provide analysis of the contemporary market of education\\nin robotics, and highlight the reasoning beyond certain design solutions.   \n",
              "39  Two-dimensional van der Waals crystals arise limitless scope for designing\\nnovel combinations of physical properties via controlling the stacking order or\\ntwist angle of individual layers. Lattice orientation between stacked\\nmonolayers is significant not only for the engineering symmetry breaking but\\nalso for the study of many-body quantum phases and band topology. So far the\\nstate-of-art exfoliation approaches focusing on achievements of quality, size,\\nyield, and scalability while lacking sufficient information on lattice\\norientation. Consequently, interlayer alignment is usually determined by later\\nexperiments, such as second harmonic generation spectroscopy, which increased\\nthe number of trials and errors for a designed artificial ordering and hampered\\nthe efficiency of systematic study. Herein, we report a lattice orientation\\ndistinguishable exfoliation method via gold favor epitaxy along the specific\\natomic step edges, meanwhile, fulfill requirements of high-quality, large-size,\\nand high-yield of monolayers. Hexagonal- and rhombohedral-stacking\\nconfigurations of bilayer transition metal dichalcogenides are built directly\\nat once result of foreseeing the lattice orientation. Optical spectroscopy,\\nelectron diffraction, and angle-resolved photoemission spectroscopy are used to\\nstudy crystals quality, symmetric breaking, and band tuning, which supports the\\nexfoliating mechanism we proposed. This strategy shows the ability for\\nfacilitates the development of ordering stacking especially for multilayers\\nassembling in the future.   \n",
              "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               We investigate the electrostrictive response across a ferroelectric phase\\ntransition from first-principles calculations and refute the prevailing view of\\nconstant electrostriction across the ferroelectric phase boundary. We take as a\\ncase study the epitaxial strain-induced transition from para- to\\nferoelectricity of \\ce{KTaO3}. We show that the magnitude of the\\nelectrostriction diverges with the permitivity at the transition, hence\\nexhibiting giant responses through a calculation of both the M and Q\\nelectrostrictive tensors. We explain the origin of this giant electrostrictive\\nresponse in \\ce{KTaO3} using a microscopic decomposition of the\\nelectrostriction coefficients, and use this understanding to propose design\\nrules for the development of future giant electrostrictors for\\nelectromechanical applications. Finally, we introduce a further means to\\ncalculate electrostriction, specific to ferroelectrics, and not yet utilised in\\nthe literature.   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 vectors  \n",
              "29            [[-0.236161, -0.4842389, 0.0925746, -0.07774249, -0.07991611, -0.18154863, -0.41444185, -0.22775541, 0.1749243, -0.8803055, -0.45890376, 0.25801623, -0.85899967, 0.3324012, -0.23452094, -0.1298467, 0.6363226, 0.24512428, -0.5664808, 0.054618478, -0.32704422, -0.586564, 0.7141909, 0.23041968, -0.15141222, 0.0059427447, 0.0816229, -0.24885096, 0.6825906, 0.4752382, -0.30978873, 0.5801748, -0.20872025, -1.1396799, 0.95265734, -0.007897456, -0.060128547, 0.020833412, -0.0019855609, 0.3989174, -0.42219692, 0.099818744, -0.053829733, -0.4455673, -0.6560595, 0.24160962, -4.639359, 0.2279825, -0.20189564, -0.5140291, -0.15982322, -0.29982156, -0.54390985, 0.7663628, 0.40060616, 0.443096, 0.14600196, 0.048438936, 0.3375926, -0.12935577, -0.10699954, 0.053393744, -0.039464153, -0.2490884, -0.36937538, 0.3675324, 0.55388224, -0.08764767, -0.7895873, 0.6328445, -0.26003048, -0.18015121, -0.39249414, -0.031190842, -0.031396683, -0.122955956, -0.39149803, 1.014085, -0.19506167, -0.25712845, -0.27542672, 0.51513153, 0.55870825, -0.12667172, 0.18538882, 0.95122904, -0.13635805, 1.0448061, -0.77243644, 0.8134837, -0.628605, -0.07683703, 0.012518975, 0.4178267, 1.0158864, 0.09594861, -0.017912274, -0.19619104, 0.41450176, -0.23265015, ...]]  \n",
              "68                  [[-0.58440727, -0.35251257, -0.46130186, -0.3444272, 0.09867792, -0.3881418, -0.28208476, -0.4390393, 0.19758092, -0.6190356, -0.16789897, 0.23644914, -0.15305749, 0.16598012, 0.07355644, 0.5806649, 0.30862865, 0.08888419, 0.2995269, 0.10089893, -0.38394114, -0.85227525, 0.6910428, 0.5453134, -0.10418466, -0.40758443, -0.4210017, -0.25388792, -0.11041971, 0.23621723, 0.11997562, 0.71480036, -0.79785216, -1.2431027, 0.6238569, 0.37358156, 0.0004435606, -0.087844975, 0.16609232, 0.54392344, -0.59622467, 0.3476366, 0.3422947, -0.38416412, 0.21641552, -0.20483942, -3.8928308, -0.0594279, -0.41725618, -0.23213756, 0.042870402, -0.10260143, -0.21709688, 0.50021493, 0.06585268, 0.42004052, 0.16631107, 0.17467724, 0.120814934, -0.26505366, -0.009890163, -0.066624105, 0.019974526, -0.60235184, 0.15099035, 0.48184317, 0.45079598, -0.3554421, -0.74269956, 0.44856027, -0.21099184, 0.2783181, -0.22401838, 0.18231075, 0.20946641, -0.41564068, -0.23265274, 0.8100551, -0.5748768, 0.5072787, 0.24838616, 0.537522, 0.12759365, -0.37195584, 0.47038257, 1.0439553, 0.21193919, 0.3802668, -0.7647531, 0.95890576, -0.58293223, 0.057352625, -0.057331942, 0.4737875, 0.94253355, 0.39002597, 0.32990748, -0.07180818, -0.21668704, 0.5699709, ...]]  \n",
              "9   [[-0.4646154, -0.30268115, -0.20571248, -0.0064003747, 0.023437861, -0.36497834, 0.19819492, -0.047951337, 0.24997261, -0.622975, -0.053030342, -0.28647768, 0.3160624, 0.5113645, 0.24822466, 0.35003236, -0.22627352, 0.30731055, 0.59284645, -0.056413632, -0.36979696, -1.0251175, 1.0525059, 0.5490411, -0.06566482, -0.48404098, 0.018295478, 0.06615573, -0.35394132, 0.3451808, -0.29777986, 0.28192753, -0.6834426, -1.0335103, 0.4205192, -0.01435646, -0.10936873, 0.15414493, 0.12959667, 0.72538763, -0.08108983, 0.5763462, 0.056264337, -0.20388089, 0.19240278, -0.37448916, -3.8809404, -0.06903507, -0.30893347, -0.21763514, 0.30224296, -0.12682638, 0.26146895, 0.18208475, 0.21749207, 0.31311166, -0.09822921, 0.0051905736, -0.12366391, -0.019480975, -0.07787055, 0.39756167, -0.020440461, -0.47621852, 0.009960584, 0.30329424, 0.46345243, 0.108682655, -0.56520957, 0.5057393, -0.6371699, 0.115991816, 0.0468635, 0.023197655, 0.21848586, -0.24467175, -0.02670729, 0.7673535, -0.11895901, 0.50123066, 0.24297804, 0.34842828, 0.43436676, -0.19790295, -0.04782945, 1.0020846, -0.32292578, -0.1107746, -0.63960713, 0.47973067, -0.57157904, -0.116786964, 0.053733334, 0.95642626, 0.8846509, 0.14435542, 0.14328793, 0.00837106, 0.034758028, 0.76816535, ...]]  \n",
              "39                     [[-0.70599306, -0.24785878, 0.039468996, 0.14890853, -0.43820912, -0.035593476, 0.04388713, -0.28340635, -0.028172104, -0.95114815, -0.09915672, 0.62737805, -0.60491776, 0.35982665, -0.2470575, 0.4915834, 0.6258402, 0.6089505, -0.3023246, -0.17517024, -0.16607867, -0.28323954, 0.84293467, 0.40134147, -0.23453239, -0.78198856, -0.20816705, -0.14589857, 0.38908547, 0.41722494, -0.24087954, 0.8238194, -0.5471987, -1.4100002, 0.70880616, -0.14400654, 0.5252719, -0.22020216, 0.3026547, 0.11455172, -0.33984977, 0.17066592, 0.79812145, -0.5577269, -0.6952416, -0.14624579, -4.3980274, 0.56230944, -0.2815758, -0.69619435, -0.12898591, -0.24546193, 0.0032363413, 1.0484098, 0.27066135, 1.014577, -0.11418613, 0.0988659, 0.34909195, 0.22792876, 0.12947713, 0.3156449, -0.3640194, -0.32399032, -0.019452192, 0.30343065, -0.13274755, -0.27157304, -0.7397675, 0.64031786, 0.19283628, -0.0771467, -0.41959763, -0.22091918, -0.20196375, -0.1417311, -0.61467135, 0.57964814, -1.0186657, -0.6687699, 0.10282913, 0.7608988, 0.13703245, 0.072465494, 0.24159198, 1.100173, -0.07438556, 0.7529132, -1.0794414, 0.8714396, -0.33225584, 0.06750364, 0.06092105, 0.5752532, 0.41503683, -0.5947066, -0.041159593, -0.4104273, 0.69378823, 0.5544189, ...]]  \n",
              "3                               [[-0.64435464, -0.28619003, -0.44010228, -0.27465537, -0.4732348, -0.35658294, 0.25145605, -0.26831272, 0.10320568, -0.5739484, -0.40173182, 0.24486767, -0.69369847, 0.34284076, 0.29456598, 0.28508773, 0.2377598, 0.35613215, -0.27831018, 0.2232181, -0.19757028, -0.48346984, 0.77740043, 0.2596198, -0.032154255, -0.4626702, -0.34772435, -0.42697367, 0.19285385, 0.14954862, -0.18999779, 1.015217, -0.5507086, -0.8568908, 0.7587826, 0.03800961, 0.32771102, 0.15347065, 0.4017725, 0.61163414, -0.6033046, -0.20775218, 0.38731426, -0.21440436, -0.7128346, -0.40726402, -3.888046, -0.19448197, -0.3612482, -0.9547592, -0.3933556, -0.06828387, 0.078413986, 0.97831124, 0.04543038, 0.4987649, -0.2917163, 0.067722626, 0.4449987, 0.1490728, 0.21572562, 0.15972847, -0.5218088, -0.44487193, 0.11286121, 1.0592451, 0.37340787, 0.071595594, -0.478459, 0.9127938, -0.29149294, -0.33697313, -0.09222529, 0.03998369, 0.19371058, 0.17288437, -0.1501307, 0.5488036, -0.4739059, -0.15548363, -0.21886173, 0.96234775, 0.5640274, -0.607868, 0.35823137, 0.9860763, -0.15157284, 0.4725818, -0.90230364, 0.93704987, -0.6518206, 0.19184595, -0.14944944, 0.12180255, 0.86806935, -0.091315776, 0.010893122, -0.40067562, 0.307744, 0.102550864, ...]]  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d08a8974-021c-477a-851b-b081ac809da1\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>abstract</th>\n",
              "      <th>vectors</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>29</td>\n",
              "      <td>We study the training of Vision Transformers for semi-supervised image\\nclassification. Transformers have recently demonstrated impressive performance\\non a multitude of supervised learning tasks. Surprisingly, we find Vision\\nTransformers perform poorly on a semi-supervised ImageNet setting. In contrast,\\nConvolutional Neural Networks (CNNs) achieve superior results in small labeled\\ndata regime. Further investigation reveals that the reason is CNNs have strong\\nspatial inductive bias. Inspired by this observation, we introduce a joint\\nsemi-supervised learning framework, Semiformer, which contains a Transformer\\nbranch, a Convolutional branch and a carefully designed fusion module for\\nknowledge sharing between the branches. The Convolutional branch is trained on\\nthe limited supervised data and generates pseudo labels to supervise the\\ntraining of the transformer branch on unlabeled data. Extensive experiments on\\nImageNet demonstrate that Semiformer achieves 75.5\\% top-1 accuracy,\\noutperforming the state-of-the-art. In addition, we show Semiformer is a\\ngeneral framework which is compatible with most modern Transformer and\\nConvolutional neural architectures.</td>\n",
              "      <td>[[-0.236161, -0.4842389, 0.0925746, -0.07774249, -0.07991611, -0.18154863, -0.41444185, -0.22775541, 0.1749243, -0.8803055, -0.45890376, 0.25801623, -0.85899967, 0.3324012, -0.23452094, -0.1298467, 0.6363226, 0.24512428, -0.5664808, 0.054618478, -0.32704422, -0.586564, 0.7141909, 0.23041968, -0.15141222, 0.0059427447, 0.0816229, -0.24885096, 0.6825906, 0.4752382, -0.30978873, 0.5801748, -0.20872025, -1.1396799, 0.95265734, -0.007897456, -0.060128547, 0.020833412, -0.0019855609, 0.3989174, -0.42219692, 0.099818744, -0.053829733, -0.4455673, -0.6560595, 0.24160962, -4.639359, 0.2279825, -0.20189564, -0.5140291, -0.15982322, -0.29982156, -0.54390985, 0.7663628, 0.40060616, 0.443096, 0.14600196, 0.048438936, 0.3375926, -0.12935577, -0.10699954, 0.053393744, -0.039464153, -0.2490884, -0.36937538, 0.3675324, 0.55388224, -0.08764767, -0.7895873, 0.6328445, -0.26003048, -0.18015121, -0.39249414, -0.031190842, -0.031396683, -0.122955956, -0.39149803, 1.014085, -0.19506167, -0.25712845, -0.27542672, 0.51513153, 0.55870825, -0.12667172, 0.18538882, 0.95122904, -0.13635805, 1.0448061, -0.77243644, 0.8134837, -0.628605, -0.07683703, 0.012518975, 0.4178267, 1.0158864, 0.09594861, -0.017912274, -0.19619104, 0.41450176, -0.23265015, ...]]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>68</th>\n",
              "      <td>68</td>\n",
              "      <td>Simultaneous wireless information and power transfer (SWIPT) has long been\\nproposed as a key solution for charging and communicating with low-cost and\\nlow-power devices. However, the employment of radio frequency (RF) signals for\\ninformation/power transfer needs to comply with international health and safety\\nregulations. In this paper, we provide a complete framework for the design and\\nanalysis of far-field SWIPT under safety constraints. In particular, we deal\\nwith two RF exposure regulations, namely, the specific absorption rate (SAR)\\nand the maximum permissible exposure (MPE). The state-of-the-art regarding SAR\\nand MPE is outlined together with a description as to how these can be modeled\\nin the context of communication networks. We propose a deep learning approach\\nfor the design of robust beamforming subject to specific information, energy\\nharvesting and SAR constraints. Furthermore, we present a thorough analytical\\nstudy for the performance of large-scale SWIPT systems, in terms of information\\nand energy coverage under MPE constraints. This work provides insights with\\nregards to the optimal SWIPT design as well as the potentials from the proper\\ndevelopment of SWIPT systems under health and safety restrictions.</td>\n",
              "      <td>[[-0.58440727, -0.35251257, -0.46130186, -0.3444272, 0.09867792, -0.3881418, -0.28208476, -0.4390393, 0.19758092, -0.6190356, -0.16789897, 0.23644914, -0.15305749, 0.16598012, 0.07355644, 0.5806649, 0.30862865, 0.08888419, 0.2995269, 0.10089893, -0.38394114, -0.85227525, 0.6910428, 0.5453134, -0.10418466, -0.40758443, -0.4210017, -0.25388792, -0.11041971, 0.23621723, 0.11997562, 0.71480036, -0.79785216, -1.2431027, 0.6238569, 0.37358156, 0.0004435606, -0.087844975, 0.16609232, 0.54392344, -0.59622467, 0.3476366, 0.3422947, -0.38416412, 0.21641552, -0.20483942, -3.8928308, -0.0594279, -0.41725618, -0.23213756, 0.042870402, -0.10260143, -0.21709688, 0.50021493, 0.06585268, 0.42004052, 0.16631107, 0.17467724, 0.120814934, -0.26505366, -0.009890163, -0.066624105, 0.019974526, -0.60235184, 0.15099035, 0.48184317, 0.45079598, -0.3554421, -0.74269956, 0.44856027, -0.21099184, 0.2783181, -0.22401838, 0.18231075, 0.20946641, -0.41564068, -0.23265274, 0.8100551, -0.5748768, 0.5072787, 0.24838616, 0.537522, 0.12759365, -0.37195584, 0.47038257, 1.0439553, 0.21193919, 0.3802668, -0.7647531, 0.95890576, -0.58293223, 0.057352625, -0.057331942, 0.4737875, 0.94253355, 0.39002597, 0.32990748, -0.07180818, -0.21668704, 0.5699709, ...]]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>9</td>\n",
              "      <td>In this paper, we present a robot model and code base for affordable\\neducation in the field of humanoid robotics. We give an overview of the\\nsoftware and hardware of a robot that won several competitions with the team\\nRoboKit in 2019-2021, provide analysis of the contemporary market of education\\nin robotics, and highlight the reasoning beyond certain design solutions.</td>\n",
              "      <td>[[-0.4646154, -0.30268115, -0.20571248, -0.0064003747, 0.023437861, -0.36497834, 0.19819492, -0.047951337, 0.24997261, -0.622975, -0.053030342, -0.28647768, 0.3160624, 0.5113645, 0.24822466, 0.35003236, -0.22627352, 0.30731055, 0.59284645, -0.056413632, -0.36979696, -1.0251175, 1.0525059, 0.5490411, -0.06566482, -0.48404098, 0.018295478, 0.06615573, -0.35394132, 0.3451808, -0.29777986, 0.28192753, -0.6834426, -1.0335103, 0.4205192, -0.01435646, -0.10936873, 0.15414493, 0.12959667, 0.72538763, -0.08108983, 0.5763462, 0.056264337, -0.20388089, 0.19240278, -0.37448916, -3.8809404, -0.06903507, -0.30893347, -0.21763514, 0.30224296, -0.12682638, 0.26146895, 0.18208475, 0.21749207, 0.31311166, -0.09822921, 0.0051905736, -0.12366391, -0.019480975, -0.07787055, 0.39756167, -0.020440461, -0.47621852, 0.009960584, 0.30329424, 0.46345243, 0.108682655, -0.56520957, 0.5057393, -0.6371699, 0.115991816, 0.0468635, 0.023197655, 0.21848586, -0.24467175, -0.02670729, 0.7673535, -0.11895901, 0.50123066, 0.24297804, 0.34842828, 0.43436676, -0.19790295, -0.04782945, 1.0020846, -0.32292578, -0.1107746, -0.63960713, 0.47973067, -0.57157904, -0.116786964, 0.053733334, 0.95642626, 0.8846509, 0.14435542, 0.14328793, 0.00837106, 0.034758028, 0.76816535, ...]]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>39</td>\n",
              "      <td>Two-dimensional van der Waals crystals arise limitless scope for designing\\nnovel combinations of physical properties via controlling the stacking order or\\ntwist angle of individual layers. Lattice orientation between stacked\\nmonolayers is significant not only for the engineering symmetry breaking but\\nalso for the study of many-body quantum phases and band topology. So far the\\nstate-of-art exfoliation approaches focusing on achievements of quality, size,\\nyield, and scalability while lacking sufficient information on lattice\\norientation. Consequently, interlayer alignment is usually determined by later\\nexperiments, such as second harmonic generation spectroscopy, which increased\\nthe number of trials and errors for a designed artificial ordering and hampered\\nthe efficiency of systematic study. Herein, we report a lattice orientation\\ndistinguishable exfoliation method via gold favor epitaxy along the specific\\natomic step edges, meanwhile, fulfill requirements of high-quality, large-size,\\nand high-yield of monolayers. Hexagonal- and rhombohedral-stacking\\nconfigurations of bilayer transition metal dichalcogenides are built directly\\nat once result of foreseeing the lattice orientation. Optical spectroscopy,\\nelectron diffraction, and angle-resolved photoemission spectroscopy are used to\\nstudy crystals quality, symmetric breaking, and band tuning, which supports the\\nexfoliating mechanism we proposed. This strategy shows the ability for\\nfacilitates the development of ordering stacking especially for multilayers\\nassembling in the future.</td>\n",
              "      <td>[[-0.70599306, -0.24785878, 0.039468996, 0.14890853, -0.43820912, -0.035593476, 0.04388713, -0.28340635, -0.028172104, -0.95114815, -0.09915672, 0.62737805, -0.60491776, 0.35982665, -0.2470575, 0.4915834, 0.6258402, 0.6089505, -0.3023246, -0.17517024, -0.16607867, -0.28323954, 0.84293467, 0.40134147, -0.23453239, -0.78198856, -0.20816705, -0.14589857, 0.38908547, 0.41722494, -0.24087954, 0.8238194, -0.5471987, -1.4100002, 0.70880616, -0.14400654, 0.5252719, -0.22020216, 0.3026547, 0.11455172, -0.33984977, 0.17066592, 0.79812145, -0.5577269, -0.6952416, -0.14624579, -4.3980274, 0.56230944, -0.2815758, -0.69619435, -0.12898591, -0.24546193, 0.0032363413, 1.0484098, 0.27066135, 1.014577, -0.11418613, 0.0988659, 0.34909195, 0.22792876, 0.12947713, 0.3156449, -0.3640194, -0.32399032, -0.019452192, 0.30343065, -0.13274755, -0.27157304, -0.7397675, 0.64031786, 0.19283628, -0.0771467, -0.41959763, -0.22091918, -0.20196375, -0.1417311, -0.61467135, 0.57964814, -1.0186657, -0.6687699, 0.10282913, 0.7608988, 0.13703245, 0.072465494, 0.24159198, 1.100173, -0.07438556, 0.7529132, -1.0794414, 0.8714396, -0.33225584, 0.06750364, 0.06092105, 0.5752532, 0.41503683, -0.5947066, -0.041159593, -0.4104273, 0.69378823, 0.5544189, ...]]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>We investigate the electrostrictive response across a ferroelectric phase\\ntransition from first-principles calculations and refute the prevailing view of\\nconstant electrostriction across the ferroelectric phase boundary. We take as a\\ncase study the epitaxial strain-induced transition from para- to\\nferoelectricity of \\ce{KTaO3}. We show that the magnitude of the\\nelectrostriction diverges with the permitivity at the transition, hence\\nexhibiting giant responses through a calculation of both the M and Q\\nelectrostrictive tensors. We explain the origin of this giant electrostrictive\\nresponse in \\ce{KTaO3} using a microscopic decomposition of the\\nelectrostriction coefficients, and use this understanding to propose design\\nrules for the development of future giant electrostrictors for\\nelectromechanical applications. Finally, we introduce a further means to\\ncalculate electrostriction, specific to ferroelectrics, and not yet utilised in\\nthe literature.</td>\n",
              "      <td>[[-0.64435464, -0.28619003, -0.44010228, -0.27465537, -0.4732348, -0.35658294, 0.25145605, -0.26831272, 0.10320568, -0.5739484, -0.40173182, 0.24486767, -0.69369847, 0.34284076, 0.29456598, 0.28508773, 0.2377598, 0.35613215, -0.27831018, 0.2232181, -0.19757028, -0.48346984, 0.77740043, 0.2596198, -0.032154255, -0.4626702, -0.34772435, -0.42697367, 0.19285385, 0.14954862, -0.18999779, 1.015217, -0.5507086, -0.8568908, 0.7587826, 0.03800961, 0.32771102, 0.15347065, 0.4017725, 0.61163414, -0.6033046, -0.20775218, 0.38731426, -0.21440436, -0.7128346, -0.40726402, -3.888046, -0.19448197, -0.3612482, -0.9547592, -0.3933556, -0.06828387, 0.078413986, 0.97831124, 0.04543038, 0.4987649, -0.2917163, 0.067722626, 0.4449987, 0.1490728, 0.21572562, 0.15972847, -0.5218088, -0.44487193, 0.11286121, 1.0592451, 0.37340787, 0.071595594, -0.478459, 0.9127938, -0.29149294, -0.33697313, -0.09222529, 0.03998369, 0.19371058, 0.17288437, -0.1501307, 0.5488036, -0.4739059, -0.15548363, -0.21886173, 0.96234775, 0.5640274, -0.607868, 0.35823137, 0.9860763, -0.15157284, 0.4725818, -0.90230364, 0.93704987, -0.6518206, 0.19184595, -0.14944944, 0.12180255, 0.86806935, -0.091315776, 0.010893122, -0.40067562, 0.307744, 0.102550864, ...]]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d08a8974-021c-477a-851b-b081ac809da1')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d08a8974-021c-477a-851b-b081ac809da1 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d08a8974-021c-477a-851b-b081ac809da1');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langdetect"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nv81VTX6TyZD",
        "outputId": "6e33eb54-fe79-458e-9861-af43410c3657"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.8/dist-packages (1.0.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from langdetect) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langdetect import detect, DetectorFactory\n",
        "DetectorFactory.seed = 0\n",
        "def translate_text(text, text_lang, target_lang='en'):\n",
        "  # Get the name of the model\n",
        "  model_name = f\"Helsinki-NLP/opus-mt-{text_lang}-{target_lang}\"\n",
        "  # Get the tokenizer\n",
        "  tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
        "\n",
        " # Instantiate the model\n",
        "  model = MarianMTModel.from_pretrained(model_name)\n",
        " \n",
        "  # Translation of the text\n",
        "  formated_text = \">>{}<< {}\".format(text_lang, text)\n",
        "  translation = model.generate(**tokenizer([formated_text], \n",
        "                               return_tensors=\"pt\", padding=True))\n",
        "  translated_text = [tokenizer.decode(t, skip_special_tokens=True) for t in       translation][0]\n",
        " \n",
        "  return translated_text\n"
      ],
      "metadata": {
        "id": "DxPCLtatTjMd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_document(text):\n",
        "  \"\"\"\n",
        "  Create a vector for given text and adjust it for cosine similarity search\n",
        "  \"\"\"\n",
        "  text_vect = create_vector_from_text(tokenizer, model, text)\n",
        "  text_vect = np.array(text_vect)\n",
        "  text_vect = text_vect.reshape(1, -1)\n",
        "  return text_vect\n",
        "\n",
        "def is_plagiarism(similarity_score, plagiarism_threshold):\n",
        "  is_plagiarism = False\n",
        "  if(similarity_score >= plagiarism_threshold):\n",
        "      is_plagiarism = True\n",
        "  return is_plagiarism\n",
        "\n",
        "def check_incoming_document(incoming_document):\n",
        "  text_lang = detect(incoming_document)\n",
        "  language_list = ['de', 'fr', 'el', 'ja', 'ru']\n",
        "  final_result = \"\"\n",
        "  if(text_lang == 'en'):\n",
        "    final_result = incoming_document\n",
        "  elif(text_lang not in language_list):\n",
        "    final_result = None\n",
        "  else:\n",
        "    # Translate in English\n",
        "    final_result = translate_text(incoming_document, text_lang)\n",
        "  return final_result\n",
        " \n",
        "def run_plagiarism_analysis(query_text, data, plagiarism_threshold=0.8):\n",
        "  top_N=3\n",
        "  # Check the language of the query/incoming text and translate if required.\n",
        "  document_translation = check_incoming_document(query_text)\n",
        "  if(document_translation is None):\n",
        "    print(\"Only the following languages are supported: English, French, Russian, German, Greek and Japanese\")\n",
        "    exit(-1)\n",
        "  else:\n",
        "    # Preprocess the document to get the required vector for similarity analysis\n",
        "    query_vect = process_document(document_translation)\n",
        "\n",
        "    # Run similarity Search\n",
        "    data[\"similarity\"] = data[\"vectors\"].apply(lambda x:\n",
        "                                            cosine_similarity(query_vect, x))\n",
        "    data[\"similarity\"] = data[\"similarity\"].apply(lambda x: x[0][0])\n",
        "    similar_articles = data.sort_values(by='similarity',\n",
        "                                        ascending=False)[1:top_N+1]\n",
        "    formated_result = similar_articles[[\"abstract\", \"id\",\n",
        "                                        \"similarity\"]].reset_index(drop = True)\n",
        "    similarity_score = formated_result.iloc[0][\"similarity\"]\n",
        "    most_similar_article = formated_result.iloc[0][\"abstract\"]\n",
        "    is_plagiarism_bool = is_plagiarism(similarity_score, plagiarism_threshold)\n",
        "    plagiarism_decision = {'similarity_score': similarity_score,\n",
        "                          'is_plagiarism': is_plagiarism_bool,\n",
        "                          'most_similar_article': most_similar_article,\n",
        "                          'article_submitted': query_text\n",
        "                          }\n",
        "    return plagiarism_decision\n"
      ],
      "metadata": {
        "id": "FsqgBNTSpl2q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "metadata": {
        "id": "Pyq3sCP3UWsG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#english_article_to_check = \"The need for multidisciplinary research to address today's complex health and environmental challenges has never been greater. The One Health (OH) approach to research ensures that human, animal, and environmental health questions are evaluated in an integrated and holistic manner to provide a more comprehensive understanding of the problem and potential solutions than would be possible with siloed approaches. However, the OH approach is complex, and there is limited guidance available for investigators regarding the practical design and implementation of OH research. In this paper we provide a framework to guide researchers through conceptualizing and planning an OH study. We discuss key steps in designing an OH study, including conceptualization of hypotheses and study aims, identification of collaborators for a multi-disciplinary research team, study design options, data sources and collection methods, and analytical methods. We illustrate these concepts through the presentation of a case study of health impacts associated with land application of biosolids. Finally, we discuss opportunities for applying an OH approach to identify solutions to current global health issues, and the need for cross-disciplinary funding sources to foster an OH approach to research.\"\n",
        "\n",
        "# Select an existing article from the data\n",
        "new_incoming_text = data.iloc[10]['abstract']\n",
        " \n",
        "# Run the plagiarism detection\n",
        "analysis_result = run_plagiarism_analysis(new_incoming_text, \n",
        "                                         vector_index, plagiarism_threshold=0.8)\n"
      ],
      "metadata": {
        "id": "AXwRNnaNT5Si"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "analysis_result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EKf2qnRoUFcf",
        "outputId": "fbf6d6d2-ce90-47a0-8b4b-992df5b9e17a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'similarity_score': 0.8836113,\n",
              " 'is_plagiarism': True,\n",
              " 'most_similar_article': 'This paper develops an efficient index modulation (IM) approach for the joint\\nradar-communication (JRC) system based on a multi-carrier multiple-input\\nmultiple-output (MIMO) radar. The communication information is embedded into\\nthe transmitted radar pulses by selecting the corresponding indices of the\\ncarrier frequencies and antenna allocations, providing two degrees of freedom.\\nOur contribution involves the development of a novel codebook based minimum\\nEuclidean distance (MED) maximization and a constellation randomization\\npre-scaling (CRPS) scheme for efficient IM-JRC transmission. It can be inferred\\nthat the IM approach integrating the CRPS scheme followed by the codebook\\ndesign maximizes the signal-to-noise ratio gain. The numerical results support\\nthe effectiveness of the proposed approach and show enhanced bit error rate\\nperformance when compared to the existing baseline.',\n",
              " 'article_submitted': 'Although nanorobots have been used as clinical prescriptions for work such as\\ngastroscopy, and even photoacoustic tomography technology has been proposed to\\ncontrol nanorobots to deliver drugs at designated delivery points in real time,\\nand there are cases of eliminating \"superbacteria\" in blood through nanorobots,\\nmost technologies are immature, either with low efficiency or low accuracy,\\nEither it can not be mass produced, so the most effective way to treat cancer\\ndiseases at this stage is through chemotherapy and radiotherapy. Patients are\\nsuffering and can not be cured. Therefore, this paper proposes an ideal model\\nof a treatment method that can completely cure cancer, a cooperative treatment\\nmethod based on nano robot queue through team member communication and computer\\nvision image classification (target detection).'}"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_incoming_text = \"You should go and love yourself\"\n",
        "\n",
        "# Run the plagiarism detection\n",
        "analysis_result = run_plagiarism_analysis(new_incoming_text, \n",
        "                                         vector_index, plagiarism_threshold=0.85)\n",
        "analysis_result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GeJaZugRVIx0",
        "outputId": "3defb90e-bf7a-4593-9291-ad1894293f85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'similarity_score': 0.6652988,\n",
              " 'is_plagiarism': False,\n",
              " 'most_similar_article': 'Although nanorobots have been used as clinical prescriptions for work such as\\ngastroscopy, and even photoacoustic tomography technology has been proposed to\\ncontrol nanorobots to deliver drugs at designated delivery points in real time,\\nand there are cases of eliminating \"superbacteria\" in blood through nanorobots,\\nmost technologies are immature, either with low efficiency or low accuracy,\\nEither it can not be mass produced, so the most effective way to treat cancer\\ndiseases at this stage is through chemotherapy and radiotherapy. Patients are\\nsuffering and can not be cured. Therefore, this paper proposes an ideal model\\nof a treatment method that can completely cure cancer, a cooperative treatment\\nmethod based on nano robot queue through team member communication and computer\\nvision image classification (target detection).',\n",
              " 'article_submitted': 'You should go and love yourself'}"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3FnoOT7OWKig"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}