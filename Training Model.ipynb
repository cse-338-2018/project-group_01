{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZleKAebgoB42",
        "outputId": "3318d5ab-c322-423c-aeae-04af948aeeb6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tflearn\n",
            "  Downloading tflearn-0.5.0.tar.gz (107 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.3/107.3 KB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from tflearn) (1.21.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from tflearn) (1.15.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.8/dist-packages (from tflearn) (7.1.2)\n",
            "Building wheels for collected packages: tflearn\n",
            "  Building wheel for tflearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tflearn: filename=tflearn-0.5.0-py3-none-any.whl size=127299 sha256=07b96aa5d6464cfd6260f52f0fcca87e7cb0b491519447857fb27947649a32c9\n",
            "  Stored in directory: /root/.cache/pip/wheels/65/9b/15/cb1e6b279c14ed897530d15cfd7da8e3df8a947e593f5cfe59\n",
            "Successfully built tflearn\n",
            "Installing collected packages: tflearn\n",
            "Successfully installed tflearn-0.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install tflearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oFJKRLCDnvI5",
        "outputId": "8703402d-5562-4dd4-89a3-289c56b33a73"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "import nltk, string\n",
        "import dill\n",
        "import pickle\n",
        "import torch\n",
        "import tflearn\n",
        "import json\n",
        "import os\n",
        "import re\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "\n",
        "nltk.download('punkt') # if necessary...\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KscriVf6w86U"
      },
      "outputs": [],
      "source": [
        "PATH = os.path.dirname(os.path.realpath('__file__'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L4jwrxTrc0qn",
        "outputId": "0072bdb0-fb3b-4885-c9ad-5c6c5b47a817"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "stemmer = nltk.stem.porter.PorterStemmer()\n",
        "remove_punctuation_map = dict((ord(char), None) for char in string.punctuation)\n",
        "\n",
        "list_words = []\n",
        "labels = []\n",
        "docs_x = [] \n",
        "docs_y = [] \n",
        "\n",
        "def cleanText(Text):\n",
        "    cleanedText = Text.replace('\\n',' ').lower()\n",
        "    cleanedText = cleanedText.replace(\"\\xc2\\xa0\", \"\")\n",
        "    cleanedText = re.sub('([,])','', cleanedText)\n",
        "    cleanedText = re.sub(' +',' ', cleanedText)\n",
        "    cleanedText = cleanedText.encode('ascii', 'ignore').decode('ascii')\n",
        "    return cleanedText\n",
        "\n",
        "def getData():\n",
        "    listTexts = source_data['abstract'].tolist()\n",
        "    finallist=[]\n",
        "    print(listTexts)\n",
        "    for i in range (0,4):\n",
        "        textDictionary = {\"tag\":i}\n",
        "        listTexts_i = cleanText(listTexts[i])\n",
        "        print(listTexts_i)\n",
        "        textDictionary.update(texts = listTexts_i)\n",
        "        finallist.append(textDictionary)\n",
        "    finalDictionary={\"intents\":finallist}       \n",
        "    return finalDictionary\n",
        "\n",
        "\n",
        "combinedDictionary = dict()\n",
        "print ('Getting Text Data')\n",
        "combinedDictionary.update(getData())\n",
        "print ('Total len of dictionary', len(combinedDictionary))\n",
        "\n",
        "print ('Saving text data dictionary')\n",
        "np.save(PATH + '/textDictionary.npy', combinedDictionary)\n",
        "\n",
        "with open(PATH + '/file.txt', 'w') as file:\n",
        "     file.write(json.dumps(combinedDictionary))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KHLzLCO6znQG"
      },
      "outputs": [],
      "source": [
        "with open(PATH + \"/file.txt\") as file:\n",
        "    dataset = json.load(file)\n",
        "    \n",
        "for intent in dataset[\"intents\"]:\n",
        "    for pattern in intent[\"texts\"]: \n",
        "        split_words = nltk.word_tokenize(pattern) \n",
        "        list_words.extend(split_words) \n",
        "        docs_x.append(split_words) \n",
        "        docs_y.append(intent[\"tag\"]) \n",
        "    if intent[\"tag\"] not in labels:\n",
        "        labels.append(intent[\"tag\"]) \n",
        "\n",
        "list_words = [stemmer.stem(w.lower()) for w in list_words if w != \"?\"] \n",
        "list_words = sorted(list(set(list_words))) \n",
        "\n",
        "labels = sorted(labels)\n",
        "\n",
        "training = [] \n",
        "output = [] \n",
        "\n",
        "out_empty = [0 for _ in range(len(labels))]\n",
        "\n",
        "for x, doc in enumerate(docs_x):\n",
        "    bag = [] \n",
        "\n",
        "    split_words = [stemmer.stem(w.lower()) for w in doc] \n",
        "\n",
        "    for w in list_words:\n",
        "        if w in split_words:\n",
        "            bag.append(1) \n",
        "        else:\n",
        "            bag.append(0)\n",
        "\n",
        "    output_row = out_empty[:]\n",
        "    output_row[labels.index(docs_y[x])] = 1 \n",
        "\n",
        "    training.append(bag) \n",
        "    output.append(output_row) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QF6FRDTJfv6t"
      },
      "outputs": [],
      "source": [
        "def stem_tokens(tokens):\n",
        "    return [stemmer.stem(item) for item in tokens]\n",
        "\n",
        "def normalize(text):\n",
        "    return stem_tokens(nltk.word_tokenize(text.lower().translate(remove_punctuation_map)))\n",
        "\n",
        "\n",
        "def bag_of_words(s, list_words): \n",
        "    bag = [0 for _ in range(len(list_words))]\n",
        "\n",
        "    inp_str_words = nltk.word_tokenize(s)\n",
        "    inp_str_words = [stemmer.stem(word.lower()) for word in inp_str_words]\n",
        "\n",
        "    for search_element in inp_str_words:\n",
        "        for i, w in enumerate(list_words):\n",
        "            if w == search_element:\n",
        "                bag[i] = 1 \n",
        "            \n",
        "    return np.array(bag)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iO_ya6zIe-11",
        "outputId": "22149338-46d4-444c-edc8-406a8fe71919"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Step: 7437  | total loss: \u001b[1m\u001b[32m1.30984\u001b[0m\u001b[0m | time: 0.929s\n",
            "\u001b[2K\r| Adam | epoch: 014 | loss: 1.30984 - acc: 0.2777 -- iter: 1984/4423\n"
          ]
        }
      ],
      "source": [
        "# Transformer Model\n",
        "training = np.array(training) \n",
        "output = np.array(output)\n",
        "\n",
        "net = tflearn.input_data(shape=[None, len(training[0])]) \n",
        "net = tflearn.fully_connected(net, 8) #Hidden layer with 8 neurons.\n",
        "net = tflearn.fully_connected(net, 8) #Another hidden layer with 8 neurons.\n",
        "net = tflearn.fully_connected(net, 8) #Another hidden layer with 8 neurons.\n",
        "net = tflearn.fully_connected(net, len(output[0]), activation=\"softmax\") \n",
        "\n",
        "model = tflearn.DNN(net) \n",
        "try:\n",
        "    model.predict(PATH + \"/model.tflearn\")\n",
        "    model.load()\n",
        "except:\n",
        "    model.fit(training, output, n_epoch=2000, batch_size=8, show_metric=True)\n",
        "    model.save(PATH + \"/model.tflearn\") "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6N6DkhFglzIW"
      },
      "outputs": [],
      "source": [
        "def get_cosine_similarity(text1, text2):\n",
        "    tfidf = TfidfVectorizer().fit_transform([text1, text2])\n",
        "    return ((tfidf * tfidf.T).A)[0,1]\n",
        "\n",
        "    \n",
        "def is_similar(similarity_score, similarity_threshold):\n",
        "\n",
        "  is_similar = False\n",
        "\n",
        "  if(similarity_score >= similarity_threshold):\n",
        "    is_similar = True\n",
        "\n",
        "  return is_similar\n",
        "\n",
        "def run_similarity_analysis(query_text, similarity_threshold=0.8):\n",
        "\n",
        "    top_N=10\n",
        "\n",
        "    # Preprocess the given document\n",
        "    preprocessed_document = preprocess_text(query_text)\n",
        "    \n",
        "    # Convert the given document into numerical features\n",
        "    X_new = tfidf.transform([preprocessed_document])\n",
        "    \n",
        "    # Compute cosine similarity between the given document and each research paper\n",
        "    similarities = cosine_similarity(training_data, X_new)\n",
        "\n",
        "    # Sort the research papers based on their similarity with the given document\n",
        "    indices = np.argsort(similarities, axis=0)\n",
        "    indices = indices[::-1]\n",
        "    \n",
        "    # Get the top N research papers with the highest similarity\n",
        "    top_n_indices = indices[:top_N, 0]\n",
        "    \n",
        "    # Compute the similarity percentage with the top 10 research papers\n",
        "    similarity_percentage = similarities[top_n_indices, 0] * 100\n",
        "\n",
        "    # assign data of lists.\n",
        "    texts = source_data['abstract'].tolist();\n",
        "    similar_articles = {'abstract': [texts[i] for i in top_n_indices], 'similarity': similarity_percentage}  \n",
        "  \n",
        "    # Create DataFrame  \n",
        "    formated_result = pd.DataFrame(similar_articles)\n",
        "    \n",
        "    # Create JSON Array\n",
        "    similarity_decision=[]\n",
        "    for x in formated_result.iloc:\n",
        "        json={'similarity_score': x[\"similarity\"], \n",
        "                           'similarity_percentage': str(round(x[\"similarity\"])) + '%',\n",
        "                            'similar_article': x[\"abstract\"]\n",
        "                        }\n",
        "        similarity_decision.append(json)\n",
        "    return similarity_decision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bn6vp9LIl35p",
        "outputId": "b393b748-f44d-48ce-fb05-ee4935fc369d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'similarity_score': 24.753818061915094,\n",
              "  'similarity_percentage': '25%',\n",
              "  'similar_article': \"How do you persuade philanthropists to pay $1 million for every pathogenic human virus you discover? Anjali Nayar talks to 'virus hunter' Nathan Wolfe in Cameroon to find out.\"},\n",
              " {'similarity_score': 21.506302762940383,\n",
              "  'similarity_percentage': '22%',\n",
              "  'similar_article': 'gas velocity dispersion measures a amount of disordered motions of the rotating disk. accurate estimates of this parameter are of a utmost importance because it was directly linked to disk stability and star formation. the global measure of a gas velocity dispersion should be inferred from a width of a atomic hydrogen hi 21 cm line. we explore how several systematic effects involved inside a production of hi cubes affect a approximate of hi velocity dispersion. we do so by comparing a hi velocity dispersion derived from different types of data cubes provided by a hi nearby galaxy survey (things). we find that residual-scaled cubes best recover a hi velocity dispersion, independent of a weighting scheme used and considering the large range of signal-to-noise ratio. considering hi observations where a dirty beam was substantially different from the gaussian, a velocity dispersion values are overestimated unless a cubes are cleaned close to (e.g., ~1.5 times) a noise level.'},\n",
              " {'similarity_score': 20.807762535043945,\n",
              "  'similarity_percentage': '21%',\n",
              "  'similar_article': 'He is so soft-spoken you may have to strain to catch his words, but on the topic of recognition for scientists in resource-poor regions, Menno de Jong is ready for a fistfight.'},\n",
              " {'similarity_score': 20.488610049787273,\n",
              "  'similarity_percentage': '20%',\n",
              "  'similar_article': \"a upcoming ooty wide field array (owfa) will operate at $326.5 \\\\, {\\\\rm mhz}$ which corresponds to a redshifted 21-cm signal from neutral hydrogen (hi) at z = 3.35. we present two different prescriptions to simulate this signal and calculate a visibilities expected inside radio-interferometric observations with owfa. inside a first method we use an input model considering a expected 21-cm power spectrum to directly simulate different random realizations of a brightness temperature fluctuations and calculate a visibilities. this method, which models a hi signal entirely as the diffuse radiation, was completely oblivious to a discrete nature of a astrophysical sources which host a hi. while each discrete source subtends an angle that was much smaller than a angular resolution of owfa, a velocity structure of a hi in a individual sources was well within reach of owfa's frequency resolution and this was expected to have an impact on a observed hi signal. a second prescription was based on cosmological n-body simulations. here we identify each simulation particle with the source that hosts a hi, and we have a freedom to implement any desired line profile considering a hi emission from a individual sources. implementing the simple model considering a line profile, we have generated several random realizations of a complex visibilities. correlations between a visibilities measured at different baselines and channels provides an unique method to quantify a statistical properties of a \\\\hi signal. we have used this to quantify a results of our simulations, and explore a relation between a expected visibility correlations and a underlying hi power spectrum.\"},\n",
              " {'similarity_score': 19.368281329076716,\n",
              "  'similarity_percentage': '19%',\n",
              "  'similar_article': '  Today, we have to deal with many data (Big data) and we need to make\\ndecisions by choosing an architectural framework to analyze these data coming\\nfrom different area. Due to this, it become problematic when we want to process\\nthese data, and even more, when it is continuous data. When you want to process\\nsome data, you have to first receive it, store it, and then query it. This is\\nwhat we call Batch Processing. It works well when you process big amount of\\ndata, but it finds its limits when you want to get fast (or real-time)\\nprocessing results, such as financial trades, sensors, user session activity,\\netc. The solution to this problem is stream processing. Stream processing\\napproach consists of data arriving record by record and rather than storing it,\\nthe processing should be done directly. Therefore, direct results are needed\\nwith a latency that may vary in real-time.\\nIn this paper, we propose an assessment quality model to evaluate and choose\\nstream processing frameworks. We describe briefly different architectural\\nframeworks such as Kafka, Spark Streaming and Flink that address the stream\\nprocessing. Using our quality model, we present a decision tree to support\\nengineers to choose a framework following the quality aspects. Finally, we\\nevaluate our model doing a case study to Twitter and Netflix streaming.\\n'},\n",
              " {'similarity_score': 19.239795952284886,\n",
              "  'similarity_percentage': '19%',\n",
              "  'similar_article': 'Being part of, or witnessing, a terrible disaster is clearly very distressing. How do we minimize this kind of distress and continue to flourish in the future as we face extreme climate events such as wildfires and flooding? How do we identify and learn from past events lying hidden, unrecognized and unforeseen as failure incubated? What are the global ‘grand challenges’—both natural and man-made? What are the UN Sustainable Development Goals? To achieve them do we need to reverse the fragmentation of the professions into ‘silos’? What is ‘joined-up’ thinking in ‘joined-up’ organisations and nation states and how important is it? Despite all the advances in science, we know less actually than we think we know. Contingency planning that expects surprises must be the new norm. Learn anew how to learn together is the new wisdom. Use the golden rule ‘do not do to others that you would not have them do to you’, whatever you are told to believe, is our tenet. Behaviour is more important than belief—imperfect doing is better than uncertain knowing. But to do all of this collectively requires leadership that we can trust—perhaps the biggest challenge of all?'},\n",
              " {'similarity_score': 18.813594134833956,\n",
              "  'similarity_percentage': '19%',\n",
              "  'similar_article': 'BACKGROUND: Human infection studies (HIS) are valuable in vaccine development. Deliberate infection, however, creates challenging questions, particularly in low and middle-income countries (LMICs) where HIS are new and ethical challenges may be heightened. Consultation with stakeholders is needed to support contextually appropriate and acceptable study design. We examined stakeholder perceptions about the acceptability and ethics of HIS in Malawi, to inform decisions about planned pneumococcal challenge research and wider understanding of HIS ethics in LMICs. METHODS: We conducted 6 deliberative focus groups and 15 follow-up interviews with research staff, medical students, and community representatives from rural and urban Blantyre. We also conducted 5 key informant interviews with clinicians, ethics committee members, and district health government officials. RESULTS: Stakeholders perceived HIS research to have potential population health benefits, but they also had concerns, particularly related to the safety of volunteers and negative community reactions. Acceptability depended on a range of conditions related to procedures for voluntary and informed consent, inclusion criteria, medical care or support, compensation, regulation, and robust community engagement. These conditions largely mirror those in existing guidelines for HIS and biomedical research in LMICs. Stakeholder perceptions pointed to potential tensions, for example, balancing equity, safety, and relevance in inclusion criteria. CONCLUSIONS: Our findings suggest HIS research could be acceptable in Malawi, provided certain conditions are in place. Ongoing assessment of participant experiences and stakeholder perceptions will be required to strengthen HIS research during development and roll-out.'},\n",
              " {'similarity_score': 17.90455200342211,\n",
              "  'similarity_percentage': '18%',\n",
              "  'similar_article': 'we examine a hi-to-stellar mass ratio (hi fraction) considering galaxies near filament backbones within a nearby universe ($d <$ 181 mpc). this work uses a 6 degree field galaxy survey and a discrete persistent structures extractor to define a filamentary structure of a local cosmic web. hi spectral stacking of hi parkes all sky survey observations yields a hi fraction considering filament galaxies and the field control sample. a hi fraction was measured considering different stellar masses and fifth nearest neighbour projected densities ($\\\\sigma_{5}$) to disentangle what influences cold gas inside galaxies. considering galaxies with stellar masses log($m_{\\\\star}$) $<$ 11 m$_{\\\\odot}$ inside projected densities 0 $\\\\leq$ $\\\\sigma_{5}$ $<$ 3 galaxies mpc$^{-2}$, all hi fractions of galaxies near filaments are statistically indistinguishable from a control sample. galaxies with stellar masses log($m_{\\\\star}$) $\\\\geq$ 11 m$_{\\\\odot}$ have the systematically higher hi fraction near filaments than a control sample. a greatest difference was 0.75 dex, which was 5.5$\\\\sigma$ difference at mean projected densities of 1.45 galaxies mpc$^{-2}$. we suggest that this was evidence considering massive galaxies accreting cold gas from a intrafilament medium that should replenish some hi gas. this supports cold mode accretion where filament galaxies with the large gravitational potential should draw gas from a large-scale structure.'},\n",
              " {'similarity_score': 17.10509114332654,\n",
              "  'similarity_percentage': '17%',\n",
              "  'similar_article': 'the method to determine a spin temperature of a local (vlsr=0 km/s) hi gas with the help of saturated brightness temperature of a 21-cm line inside a radial-velocity degenerate regions (vdr) was presented. a spin temperatures was determined to be ts= 146.2 +/- 16.1 k by measuring saturated brightness inside a vdr toward a galactic center, 146.8 +/- 10.7 k by chi^2 fitting of expected brightness distribution to observation around a vdr, and 144.4 +/- 6.8 k toward a local arm. assuming ts=146 k, the correction factor gamma considering a hi density, defined by a ratio of a true hi density considering finite optical thickness to that calculated by assuming optically thin hi, is obtained to be gamma~1.2 (optical depth tau~0.3) inside a local hi gas, ~1.8 (~1.3) toward a arm and anti-center, and as high as ~3.6 (~2.7) inside a galactic center direction. it was suggested that a hi density and mass inside a local arm could be ~2 times, and that inside a inner galaxy ~3.6 times, greater than a currently estimated values.'},\n",
              " {'similarity_score': 16.6505025053534,\n",
              "  'similarity_percentage': '17%',\n",
              "  'similar_article': 'Can you diagnose the cause of this man’s bilateral pulmonary nodules and acute respiratory failure? http://ow.ly/NfED30dDBzm'}]"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "new_incoming_text = \"hi, hello, how are you?\"\n",
        "\n",
        "# Run the plagiarism detection\n",
        "analysis_result = run_similarity_analysis(new_incoming_text, similarity_threshold=0.8)\n",
        "analysis_result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mzLvpIuwmFM3",
        "outputId": "bd19e1c5-f2d7-418c-bfd0-d783da40c5e0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'similarity_score': 100.00000000000004,\n",
              "  'similarity_percentage': '100%',\n",
              "  'similar_article': 'A critical aspect of reliable communication involves the design of codes that\\nallow transmissions to be robustly and computationally efficiently decoded\\nunder noisy conditions. Advances in the design of reliable codes have been\\ndriven by coding theory and have been sporadic. Recently, it is shown that\\nchannel codes that are comparable to modern codes can be learned solely via\\ndeep learning. In particular, Turbo Autoencoder (TURBOAE), introduced by Jiang\\net al., is shown to achieve the reliability of Turbo codes for Additive White\\nGaussian Noise channels. In this paper, we focus on applying the idea of\\nTURBOAE to various practical channels, such as fading channels and chirp noise\\nchannels. We introduce TURBOAE-TI, a novel neural architecture that combines\\nTURBOAE with a trainable interleaver design. We develop a carefully-designed\\ntraining procedure and a novel interleaver penalty function that are crucial in\\nlearning the interleaver and TURBOAE jointly. We demonstrate that TURBOAE-TI\\noutperforms TURBOAE and LTE Turbo codes for several channels of interest. We\\nalso provide interpretation analysis to better understand TURBOAE-TI.'},\n",
              " {'similarity_score': 40.2093299068417,\n",
              "  'similarity_percentage': '40%',\n",
              "  'similar_article': 'There have been significant research activities in recent years to automate\\nthe design of channel encoders and decoders via deep learning. Due the\\ndimensionality challenge in channel coding, it is prohibitively complex to\\ndesign and train relatively large neural channel codes via deep learning\\ntechniques. Consequently, most of the results in the literature are limited to\\nrelatively short codes having less than 100 information bits. In this paper, we\\nconstruct ProductAEs, a computationally efficient family of deep-learning\\ndriven (encoder, decoder) pairs, that aim at enabling the training of\\nrelatively large channel codes (both encoders and decoders) with a manageable\\ntraining complexity. We build upon the ideas from classical product codes, and\\npropose constructing large neural codes using smaller code components. More\\nspecifically, instead of directly training the encoder and decoder for a large\\nneural code of dimension $k$ and blocklength $n$, we provide a framework that\\nrequires training neural encoders and decoders for the code parameters\\n$(k_1,n_1)$ and $(k_2,n_2)$ such that $k_1 k_2=k$ and $n_1 n_2=n$. Our training\\nresults show significant gains, over all ranges of signal-to-noise ratio (SNR),\\nfor a code of parameters $(100,225)$ and a moderate-length code of parameters\\n$(196,441)$, over polar codes under successive cancellation (SC) decoder.\\nMoreover, our results demonstrate meaningful gains over Turbo Autoencoder\\n(TurboAE) and state-of-the-art classical codes. This is the first work to\\ndesign product autoencoders and a pioneering work on training large channel\\ncodes.'},\n",
              " {'similarity_score': 27.646666764071238,\n",
              "  'similarity_percentage': '28%',\n",
              "  'similar_article': 'Landmark codes underpin reliable physical layer communication, e.g.,\\nReed-Muller, BCH, Convolution, Turbo, LDPC and Polar codes: each is a linear\\ncode and represents a mathematical breakthrough. The impact on humanity is\\nhuge: each of these codes has been used in global wireless communication\\nstandards (satellite, WiFi, cellular). Reliability of communication over the\\nclassical additive white Gaussian noise (AWGN) channel enables benchmarking and\\nranking of the different codes. In this paper, we construct KO codes, a\\ncomputationaly efficient family of deep-learning driven (encoder, decoder)\\npairs that outperform the state-of-the-art reliability performance on the\\nstandardized AWGN channel. KO codes beat state-of-the-art Reed-Muller and Polar\\ncodes, under the low-complexity successive cancellation decoding, in the\\nchallenging short-to-medium block length regime on the AWGN channel. We show\\nthat the gains of KO codes are primarily due to the nonlinear mapping of\\ninformation bits directly to transmit real symbols (bypassing modulation) and\\nyet possess an efficient, high performance decoder. The key technical\\ninnovation that renders this possible is design of a novel family of neural\\narchitectures inspired by the computation tree of the {\\\\bf K}ronecker {\\\\bf\\nO}peration (KO) central to Reed-Muller and Polar codes. These architectures\\npave way for the discovery of a much richer class of hitherto unexplored\\nnonlinear algebraic structures. The code is available at\\n\\\\href{https://github.com/deepcomm/KOcodes}{https://github.com/deepcomm/KOcodes}'},\n",
              " {'similarity_score': 27.171705852081864,\n",
              "  'similarity_percentage': '27%',\n",
              "  'similar_article': 'Polar codes are normally designed based on the reliability of the\\nsub-channels in the polarized vector channel. There are various methods with\\ndiverse complexity and accuracy to evaluate the reliability of the\\nsub-channels. However, designing polar codes solely based on the sub-channel\\nreliability may result in poor Hamming distance properties. In this work, we\\npropose a different approach to design the information set for polar codes and\\nPAC codes where the objective is to reduce the number of codewords with minimum\\nweight (a.k.a. error coefficient) of a code designed for maximum reliability.\\nThis approach is based on the coset-wise characterization of the rows of polar\\ntransform $\\\\mathbf{G}_N$ involved in the formation of the minimum-weight\\ncodewords. Our analysis capitalizes on the properties of the polar transform\\nbased on its row and column indices. The numerical results show that the\\ndesigned codes outperform PAC codes and CRC-Polar codes at the practical block\\nerror rate of $10^{-2}-10^{-3}$. Furthermore, a by-product of the combinatorial\\nproperties analyzed in this paper is an alternative enumeration method of the\\nminimum-weight codewords.'},\n",
              " {'similarity_score': 26.901406514105837,\n",
              "  'similarity_percentage': '27%',\n",
              "  'similar_article': \"achieving information-theoretic security with the help of explicit coding scheme inside which unlimited computational power considering eavesdropper was assumed, was one of a main topics was security consideration. it was shown that polar codes are capacity achieving codes and have the low complexity inside encoding and decoding. it has been proven that polar codes reach to secrecy capacity inside a binary-input wiretap channels inside symmetric settings considering which a wiretapper's channel was degraded with respect to a main channel. a first task of this paper was to propose the coding scheme to achieve secrecy capacity inside asymmetric nonbinary-input channels while keeping reliability and security conditions satisfied. our assumption was that a wiretap channel was stochastically degraded with respect to a main channel and message distribution was unspecified. a main idea was to send information set over good channels considering bob and bad channels considering eve and send random symbols considering channels that are good considering both. inside this scheme a frozen vector was defined over all possible choices with the help of polar codes ensemble concept. we proved that there exists the frozen vector considering which a coding scheme satisfies reliability and security conditions. it was further shown that uniform distribution of a message was a necessary condition considering achieving secrecy capacity.\"},\n",
              " {'similarity_score': 26.082379601361644,\n",
              "  'similarity_percentage': '26%',\n",
              "  'similar_article': 'The development of optimal and efficient machine learning-based communication\\nsystems is likely to be a key enabler of beyond 5G communication technologies.\\nIn this direction, physical layer design has been recently reformulated under a\\ndeep learning framework where the autoencoder paradigm foresees the full\\ncommunication system as an end-to-end coding-decoding problem. Given the loss\\nfunction, the autoencoder jointly learns the coding and decoding optimal blocks\\nunder a certain channel model. Because performance in communications typically\\nrefers to achievable rates and channel capacity, the mutual information between\\nchannel input and output can be included in the end-to-end training process,\\nthus, its estimation becomes essential. In this paper, we present a set of\\nnovel discriminative mutual information estimators and we discuss how to\\nexploit them to design capacity-approaching codes and ultimately estimate the\\nchannel capacity.'},\n",
              " {'similarity_score': 25.589816802574777,\n",
              "  'similarity_percentage': '26%',\n",
              "  'similar_article': '  This work investigates the fundamental limits of communication over a noisy\\ndiscrete memoryless channel that wears out, in the sense of signal-dependent\\ncatastrophic failure. In particular, we consider a channel that starts as a\\nmemoryless binary-input channel and when the number of transmitted ones causes\\na sufficient amount of damage, the channel ceases to convey signals. Constant\\ncomposition codes are adopted to obtain an achievability bound and the\\nleft-concave right-convex inequality is then refined to obtain a converse bound\\non the log-volume throughput for channels that wear out. Since infinite\\nblocklength codes will always wear out the channel for any finite threshold of\\nfailure and therefore cannot convey information at positive rates, we analyze\\nthe performance of finite blocklength codes to determine the maximum expected\\ntransmission volume at a given level of average error probability. We show that\\nthis maximization problem has a recursive form and can be solved by dynamic\\nprogramming. Numerical results demonstrate that a sequence of block codes is\\npreferred to a single block code for streaming sources.\\n'},\n",
              " {'similarity_score': 24.878019669478213,\n",
              "  'similarity_percentage': '25%',\n",
              "  'similar_article': '  Cyclic codes and their various generalizations, such as quasi-twisted (QT)\\ncodes, have a special place in algebraic coding theory. Among other things,\\nmany of the best-known or optimal codes have been obtained from these classes.\\nIn this work we introduce a new generalization of QT codes that we call\\nmulti-twisted (MT) codes and study some of their basic properties. Presenting\\nseveral methods of constructing codes in this class and obtaining bounds on the\\nminimum distances, we show that there exist codes with good parameters in this\\nclass that cannot be obtained as QT or constacyclic codes. This suggests that\\nconsidering this larger class in computer searches is promising for\\nconstructing codes with better parameters than currently best-known linear\\ncodes. Working with this new class of codes motivated us to consider a problem\\nabout binomials over finite fields and to discover a result that is interesting\\nin its own right.\\n'},\n",
              " {'similarity_score': 24.85099697957587,\n",
              "  'similarity_percentage': '25%',\n",
              "  'similar_article': 'The two-user interference channel is a model for multi one-to-one\\ncommunications, where two transmitters wish to communicate with their\\ncorresponding receivers via a shared wireless medium. Two most common and\\nsimple coding schemes are time division (TD) and treating interference as noise\\n(TIN). Interestingly, it is shown that there exists an asymptotic scheme,\\ncalled Han-Kobayashi scheme, that performs better than TD and TIN. However,\\nHan-Kobayashi scheme has impractically high complexity and is designed for\\nasymptotic settings, which leads to a gap between information theory and\\npractice.\\n  In this paper, we focus on designing practical codes for interference\\nchannels. As it is challenging to analytically design practical codes with\\nfeasible complexity, we apply deep learning to learn codes for interference\\nchannels. We demonstrate that DeepIC, a convolutional neural network-based code\\nwith an iterative decoder, outperforms TD and TIN by a significant margin for\\ntwo-user additive white Gaussian noise channels with moderate amount of\\ninterference.'},\n",
              " {'similarity_score': 24.770549394330065,\n",
              "  'similarity_percentage': '25%',\n",
              "  'similar_article': 'a emerging interest inside low-latency high-reliability applications, such as connected vehicles, necessitates the new abstraction between communication and control. thanks to advances inside cyber-physical systems over a past decades, we understand this interface considering classical bit-rate models of channels as well as packet-loss-type channels. this work proposes the new abstraction characterized as the tradeoff curve between latency, reliability and rate. our aim was to understand: do we (control engineers) prefer faster but less reliable communications (with shorter codes), or slower but more reliable communications (with longer codes)? inside this paper we examine a tradeoffs between latency and reliability considering a problem of estimating dynamical systems over communication channels. employing different latency-reliability curves derived from practical coding schemes, we develop the co-design methodology, i.e., select a code length depending on a system dynamics to optimize system performance.'}]"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "new_incoming_text =  source_data.iloc[1]['abstract']\n",
        "\n",
        "# Run the plagiarism detection\n",
        "analysis_result = run_plagiarism_analysis(new_incoming_text, similarity_threshold=0.8)\n",
        "analysis_result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0zY70b1Im2hg",
        "outputId": "68af5981-25e8-4672-be46-af5bb3c1f2fc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'similarity_score': 55.90983656174052,\n",
              "  'similarity_percentage': '56%',\n",
              "  'similar_article': 'Nucleic acid-based therapeutics present huge potential in the treatment of pulmonary diseases ranging from lung cancer to asthma and chronic pulmonary diseases, which are often fatal and widely prevalent. The susceptibility of nucleic acids to degradation and the complex structure of lungs retard the effective pulmonary delivery of nucleic acid drug. To overcome these barriers, different strategies have been exploited to increase the delivery efficiency using chemically synthesized nucleic acids, vector encapsulation, proper formulation, and administration route. However, several limitations regarding off-target effects and immune stimulation of nucleic acid drugs hamper their translation into the clinical practice. Therefore, their successful clinical application will ultimately rely on well-developed carriers and methods to ensure safety and efficacy. In this review, we provide a comprehensive overview of the nucleic acid application for pulmonary diseases, covering action mechanism of the nucleic acid drugs, the novel delivery systems, and the current formulation for the administration to lungs. The latest advances of nucleic acid drugs under clinical evaluation to treat pulmonary disorders will also be detailed.'},\n",
              " {'similarity_score': 52.65197469127353,\n",
              "  'similarity_percentage': '53%',\n",
              "  'similar_article': 'Organisms throughout biology need to maintain the integrity of their genome. From bacteria to vertebrates, life has established sophisticated mechanisms to detect and eliminate foreign genetic material or to restrict its function and replication. Tremendous progress has been made in the understanding of these mechanisms which keep foreign or unwanted nucleic acids from viruses or phages in check. Mechanisms reach from restriction-modification systems and CRISPR/Cas in bacteria and archaea to RNA interference and immune sensing of nucleic acids, altogether integral parts of a system which is now appreciated as nucleic acid immunity. With inherited receptors and acquired sequence information, nucleic acid immunity comprises innate and adaptive components. Effector functions include diverse nuclease systems, intrinsic activities to directly restrict the function of foreign nucleic acids (e.g., PKR, ADAR1, IFIT1), and extrinsic pathways to alert the immune system and to elicit cytotoxic immune responses. These effects act in concert to restrict viral replication and to eliminate virus-infected cells. The principles of nucleic acid immunity are highly relevant for human disease. Besides its essential contribution to antiviral defense and restriction of endogenous retroelements, dysregulation of nucleic acid immunity can also lead to erroneous detection and response to self nucleic acids then causing sterile inflammation and autoimmunity. Even mechanisms of nucleic acid immunity which are not established in vertebrates are relevant for human disease when they are present in pathogens such as bacteria, parasites, or helminths or in pathogen-transmitting organisms such as insects. This review aims to provide an overview of the diverse mechanisms of nucleic acid immunity which mostly have been looked at separately in the past and to integrate them under the framework nucleic acid immunity as a basic principle of life, the understanding of which has great potential to advance medicine.'},\n",
              " {'similarity_score': 40.4023104830436,\n",
              "  'similarity_percentage': '40%',\n",
              "  'similar_article': 'BACKGROUND: As we enter the post-genome sequencing era and begin to sift through the enormous amount of genetic information now available, the need for technologies that allow rapid, cost-effective, high-throughput detection of specific nucleic acid sequences becomes apparent. Multiplexing technologies, which allow for simultaneous detection of multiple nucleic acid sequences in a single reaction, can greatly reduce the time, cost and labor associated with single reaction detection technologies. METHODS: The Luminex® xMAP™ system is a multiplexed microsphere-based suspension array platform capable of analyzing and reporting up to 100 different reactions in a single reaction vessel. This technology provides a new platform for high-throughput nucleic acid detection and is being utilized with increasing frequency. Here we review specific applications of xMAP technology for nucleic acid detection in the areas of single nucleotide polymorphism (SNP) genotyping, genetic disease screening, gene expression profiling, HLA DNA typing and microbial detection. CONCLUSIONS: These studies demonstrate the speed, efficiency and utility of xMAP technology for simultaneous, rapid, sensitive and specific nucleic acid detection, and its capability to meet the current and future requirements of the molecular laboratory for high-throughput nucleic acid detection.'},\n",
              " {'similarity_score': 38.30852143180135,\n",
              "  'similarity_percentage': '38%',\n",
              "  'similar_article': 'Summary: Host cells trigger signals for innate immune responses upon recognition of conserved structures in microbial pathogens. Nucleic acids, which are critical components for inheriting genetic information in all species including pathogens, are key structures sensed by the innate immune system. The corresponding receptors for foreign nucleic acids include members of Toll‐like receptors, RIG‐I‐like receptors, and intracellular DNA sensors. While nucleic acid recognition by these receptors is required for host defense against the pathogen, there is a potential risk to the host of self‐nucleic acids recognition, thus precipitating autoimmune and autoinflammatory diseases. In this review, we discuss the roles of nucleic acid‐sensing receptors in guarding against pathogen invasion, discriminating between self and non‐self, and contributing to autoimmunity and autoinflammatory diseases.'},\n",
              " {'similarity_score': 36.63843432639991,\n",
              "  'similarity_percentage': '37%',\n",
              "  'similar_article': 'Nucleic acid amplification techniques are commonly used currently to diagnose viral diseases and manage patients with this kind of illnesses. These techniques have had a rapid but unconventional route of development during the last 30 years, with the discovery and introduction of several assays in clinical diagnosis. The increase in the number of commercially available methods has facilitated the use of this technology in the majority of laboratories worldwide. This technology has reduced the use of some other techniques such as viral culture based methods and serological assays in the clinical virology laboratory. Moreover, nucleic acid amplification techniques are now the methods of reference and also the most useful assays for the diagnosis in several diseases. The introduction of these techniques and their automation provides new opportunities for the clinical laboratory to affect patient care. The main objectives in performing nucleic acid tests in this field are to provide timely results useful for high-quality patient care at a reasonable cost, because rapid results are associated with improvements in patients care. The use of amplification techniques such as polymerase chain reaction, real-time polymerase chain reaction or nucleic acid sequence-based amplification for virus detection, genotyping and quantification have some advantages like high sensitivity and reproducibility, as well as a broad dynamic range. This review is an up-to-date of the main nucleic acid techniques and their clinical applications, and special challenges and opportunities that these techniques currently provide for the clinical virology laboratory.'},\n",
              " {'similarity_score': 34.71428979479004,\n",
              "  'similarity_percentage': '35%',\n",
              "  'similar_article': 'Today, Scientific Data is refining its standards for new submissions describing nucleic acid sequence data.'},\n",
              " {'similarity_score': 34.29457989931126,\n",
              "  'similarity_percentage': '34%',\n",
              "  'similar_article': 'Quick, simple and accurate diagnosis of suspected COVID-19 is very important for the screening and therapy of patients. Although several methods were performed in clinical practice, however, the IgM and IgG diagnostic value evaluation was little performed. 57 suspected COVID-19 infection patients were enrolled in our study. 24 patients with positive and 33 patients with negative nucleic acid test. The positive rate of COVID-19 nucleic acid was 42.10%. The positive detection rate of combination of IgM and IgG for patients with COVID-19 negative and positive nucleic acid test was 72.73% and 87.50%. The results were significantly higher than the nucleic acid or IgM, IgG single detection. hsCRP in the COVID-19 nucleic acid negative group showed significantly higher than the positive groups (P=0.0298). AST in the COVID-19 IgM negative group showed significantly lower than the positive groups (P=0.0365). We suggest a quick, simple, accurate aided detection method for the suspected patients and on-site screening in close contact with the population.'},\n",
              " {'similarity_score': 34.188009862900714,\n",
              "  'similarity_percentage': '34%',\n",
              "  'similar_article': 'ince December 2019, an outbreak of the novel coronavirus (SARS-CoV-2) infection has spread rapidly in Wuhan, China [1]1. Over a month since the outbreak, more than 13,000 patients with COVID-19 have be cured and discharged from hospital until now. For the clinical cure criteria in China, twice successive negative results of Sars-Cov-2 nucleic acid detection are the important index, in addition to normal body temperature for 3 days as well as obvious improvement in respiratory symptoms and CT scan [2]. In the present work, we reported that Sars-Cov-2 nucleic acid was still detectable in sputum obtained by nebulization from a cured patient. On January 22, a 49-year-old man presented himself with fever for 4 days to a clinic. Throat swab detection was positive for SARS-CoV-2 nucleic acid by real-time RT-PCR. Subsequently, the patient was diagnosed with COVID-19 according to the diagnostic criteria [2] as follows (1): the positive result of SARS-CoV-2 nucleic acid detection (2); a history of short stay in Wuhan within 14 days; and (3) symptoms of fever, and multiple patchy areas of ground-glass opacity on CT scan. After the active treatment, the patient recovered from fever and other respiratory symptoms on February 4. On February 9 and February 10, the SARS-CoV-2 nucleic acid detection was successively negative in his throat swab samples. The CT scan result showed that the inflammation was significantly decreased in both lungs. Both the results of SARS-CoV-2 nucleic acid detection and CT scans indicated a recovery trend, and the patient was ready for discharge. On February 13, the throat swab and sputum by nebulization were collected before the patient was discharged. Notably, SARS-CoV-2 nucleic acid was still detected in sputum from the patient although negative result of throat swab detection.'},\n",
              " {'similarity_score': 34.18654874422863,\n",
              "  'similarity_percentage': '34%',\n",
              "  'similar_article': 'Nucleic acid probe technology is increasingly being used in basic research in veterinary microbiology and in diagnosis of infectious diseases of veterinary importance. This review presents an overview of nucleic acid probe methodology and its applications in veterinary infectious diseases. The major applications of nucleic acid probes include detection of pathogens in clinical samples, especially those organisms which are fastidious and difficult to cultivate, differentiation of virulent from a virulent organisms and vaccine strains from wild type isolates, typing of microorganisms mapping genes, screening libraries of cloned DNA for specific genes, detection of latently infected or carrier animals, study of mechanisms of pathogenesis, epidemiological studies and food safety.'},\n",
              " {'similarity_score': 34.09409629430082,\n",
              "  'similarity_percentage': '34%',\n",
              "  'similar_article': 'Abstract Nucleic acid probe technology is increasingly being used in basic research in veterinary microbiology and in diagnosis of infectious diseases of veterinary importance. This review presents an overview of nucleic acid probe methodology and its applications in veterinary infectious diseases. The major applications of nucleic acid probes include detection of pathogens in clinical samples, especially those organisms which are fastidious and difficult to cultivate, differentiation of virulent from a virulent organisms and vaccine strains from wild type isolates, typing of microorganisms mapping genes, screening libraries of cloned DNA for specific genes, detection of latently infected or carrier animals, study of mechanisms of pathogenesis, epidemiological studies and food safety.'}]"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pickle.dump(run_plagiarism_analysis, open('similarity_model.pkl','wb'))\n",
        "g = pickle.load(open('similarity_model.pkl','rb'))\n",
        "g(\"possible further increases in nucleic acid\", similarity_threshold=0.8)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
